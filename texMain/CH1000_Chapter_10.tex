% !TeX root = ThesisMain.tex
% !TeX program = XeLaTeX
% !TeX encoding = UTF-8
% !TeX spellcheck = en_GB

\documentclass[../ThesisMain]{subfiles}
\ifSubfilesClassLoaded{}{}%

\begin{document}
\doublespacing%
\chapter{Conclusion and Future Work}\label{chap:conclusion}

\section{Summary of Contributions}

This thesis has developed a comprehensive framework for physics-informed machine learning wall functions applicable to turbulent and transitional flows with heat transfer \cite{1701_07102_v2, raissi2019physics, 2206_05226_v2}. The research addresses a fundamental challenge in computational fluid dynamics: the accurate prediction of wall shear stress and heat flux in complex flow conditions where traditional algebraic wall functions fail \cite{launder1974, 2309_02109_v1, 2409_04143_v1}. The main contributions of this work are summarised below.

\subsection{A Unified Framework for Physics-Informed Wall Modelling}

The central contribution of this thesis is the development and systematic evaluation of three complementary approaches to incorporating physics knowledge into neural network wall functions.

\textbf{Method A: Physics-Encoded Inputs} (Chapter~\ref{chap:physics_features}) establishes a library of 58 non-dimensional feature variables derived from fluid mechanics principles \cite{ling2016, 1905_07510_v2}, transforming raw primitive variables into physics-meaningful representations that enable learning across Reynolds numbers and flow regimes \cite{2210_15384_v1, 2307_13144_v1}. This approach leverages established turbulence theory by encoding wall-law scaling, pressure gradient effects, strain rate mechanics, and thermal boundary layer structure directly into the network inputs.

\textbf{Method B: Physics-Guided Hidden Layers} (Chapter~\ref{chap:neurons}) reveals through neuron-feature correlation analysis that neural networks spontaneously learn physics-aligned representations when trained on wall function prediction tasks. Architecture-invariant features emerge consistently across networks with 8, 16, and 32 neurons, demonstrating that the learned representations encode fundamental physics rather than artifacts of specific model configurations. This finding validates the interpretability of neural networks for turbulence modelling and provides guidance for architecture design.

\textbf{Method C: Physics-Constrained Learning} (Chapter~\ref{chap:pinn}) implements local stencil-based physics-informed neural networks (PINNs) \cite{raissi2019physics, 2511_14497_v1, 2503_17704_v1} that incorporate momentum, energy, and continuity residuals computed from finite differences on the local wall stencil. This approach provides physics regularisation without requiring computationally expensive automatic differentiation through the network \cite{2205_08663_v2, 2409_19851_v1}, enabling practical deployment while ensuring predictions satisfy conservation laws.

These three methods are not mutually exclusive but rather complementary, and the thesis demonstrates how they can be combined for optimal performance.

\subsection{Dual-Mesh Training Methodology}

A key methodological contribution is the dual-mesh training approach that enables supervised learning of wall functions. The method extracts local 3$\times$5 stencils from coarse meshes with $y^+ \approx 5$--10 at the first cell, representing typical industrial CFD practice, and uses these as model inputs. The training targets are wall shear stress and heat flux from wall-resolved simulations with $y^+ < 2$, providing ground truth without requiring DNS or experimental data. This dual-mesh strategy bridges the gap between the coarse meshes used in practical simulations and the accuracy achievable with wall-resolved computations, without the prohibitive computational cost of running fine-mesh simulations at deployment time. The approach is general and can be applied to any wall-modelled quantity where high-fidelity reference data can be generated offline.

\subsection{Comprehensive Training Dataset}

The thesis establishes a diverse training dataset comprising 244 simulation cases generated through systematic parametric variation. The dataset includes 180 asymmetric diffuser configurations with expansion ratios ranging from 1.05 to 4.5 and Reynolds numbers from 6,000 to 24,000, providing extensive coverage of adverse pressure gradient conditions from mild to near-separation. To balance the dataset with favourable pressure gradients, 60 nozzle (contraction) configurations with contraction ratios from 0.5 to 0.9 were simulated, ensuring the model learns accelerating flow physics alongside decelerating flows. Four channel flow cases at different Reynolds numbers provide equilibrium boundary layer data for validation and baseline performance assessment. This dataset of 25,485 training samples extracted from wall-adjacent cells across all cases spans a wide range of flow conditions including attached flows, adverse pressure gradients, and near-separation regions, enabling robust generalisation to unseen geometries and operating conditions.

\subsection{Flow Separation Detection}

Chapter~\ref{chap:separation} extends the framework from regression to classification, developing machine learning classifiers that identify flow separation regions from local stencil data alone. This capability enables a hybrid wall modelling strategy where ML wall functions are applied in separated regions where traditional wall functions fail catastrophically, while computationally efficient traditional methods can be retained in attached regions where they perform adequately. The separation detection problem is formulated as binary classification (attached versus separated) using the same 58 physics-based features developed for the regression task, with labels assigned based on wall shear stress sign from high-fidelity simulations. The Random Forest classifier achieves 98.8\% accuracy (F1 = 0.975) in detecting near-separation conditions, demonstrating that this classification problem is fundamentally tractable with appropriate physics features despite the challenge of predicting a global flow state from purely local information.

\subsection{OpenFOAM Integration}

The practical applicability of the developed methods is demonstrated through direct integration into the OpenFOAM solver framework (Chapter~\ref{chap:openfoam}). The implementation provides custom boundary conditions for velocity and temperature that extract local stencils from the flow field, compute physics-based features, perform neural network inference, and apply the predicted wall shear stress and heat flux to enforce boundary conditions. A Python-C++ interface enables model training in PyTorch with deployment through LibTorch C++ libraries, avoiding runtime Python dependencies. The modular design supports different ML architectures (varying numbers of hidden layers and neurons), alternative feature sets (reduced feature subsets for computational efficiency), and optional physics constraints (PINN loss terms), enabling systematic performance comparison within the production solver environment.

\section{Key Findings}

\subsection{Physics-Encoded Inputs (Method A)}

The systematic evaluation of physics-based input features in Chapter~\ref{chap:physics_features} yielded several important findings:

\paragraph{Feature Engineering Dramatically Improves Performance.} Replacing the 6 primitive input variables (position, velocity, pressure, temperature) with 58 physics-based non-dimensional groups improves wall shear stress prediction from $R^2 = 0.89$ to $R^2 = 0.95$. This 6\% improvement in explained variance corresponds to a substantial reduction in prediction error, particularly in challenging flow conditions.

\paragraph{Non-Dimensional Formulation Enables Generalisation.} The physics features are constructed as non-dimensional groups following Buckingham Pi theorem principles. This ensures that the learned relationships are scale-invariant, enabling the model trained at one Reynolds number to generalise to others without retraining.

\paragraph{Feature Categories Have Different Importance.} The 58 features can be categorised by their physical origin, with each category contributing distinct information to the prediction task. Wall-distance features including $y^+$ and log-law deviation ratios are essential for capturing the boundary layer structure and identifying the appropriate turbulent regime. Velocity gradient features encompassing shear, strain rate invariants, and rotation tensors prove critical for separation detection by quantifying the deformation field responsible for boundary layer growth. Pressure gradient features serve as primary indicators of adverse flow conditions, directly measuring the driving force for separation or acceleration. Thermal features including temperature gradients and thermal wall distance are important for both heat transfer prediction and separation detection, reflecting the strong coupling between momentum and thermal boundary layers in separated flows.

\paragraph{Curated Feature Subsets Approach Full Performance.} Using only 17 separation-indicative features achieves 95\% of the performance of the full 58-feature model, suggesting that the feature library contains redundancy that could be exploited for computational efficiency.

\subsection{Physics-Guided Hidden Layers (Method B)}

The analysis of hidden layer representations in Chapter~\ref{chap:neurons} revealed unexpected insights into how neural networks learn physics:

\paragraph{Neurons Spontaneously Align with Physics Features.} Despite being trained only to minimise prediction error on wall shear stress and heat flux, hidden layer neurons develop strong correlations with physics-meaningful features. Correlation coefficients exceeding 0.8 are observed between individual neurons and features such as pressure gradient, velocity-distance ratios, and thermal indicators.

\paragraph{Architecture-Invariant Features Emerge.} Two features---the streamwise pressure gradient $\partial p/\partial x$ and the velocity-distance-viscosity ratio $u_2 y_2/\nu$---emerge as strongly correlated with hidden neurons regardless of network architecture (8, 16, 32, or 64 neurons). This architecture invariance suggests these features encode fundamental physics rather than artifacts of a particular model configuration.

\paragraph{L1-Regularised Networks Are More Interpretable.} Networks trained with L1 regularisation on the first hidden layer develop sparser, more interpretable representations. The L1-PINN architecture with 32 neurons achieves $R^2 = 0.948$ for wall shear stress while maintaining clear neuron-feature alignment, demonstrating that interpretability need not come at the cost of accuracy.

\paragraph{Neuron Replacement Validates Physics Understanding.} Replacing trained neurons with their most-correlated physics features and retraining only the output layer recovers 85--90\% of original model performance. This remarkable result confirms that the learned representations are genuinely physics-aligned rather than spuriously correlated.

\subsection{Physics-Constrained Learning (Method C)}

The PINN experiments in Chapter~\ref{chap:pinn} explored the trade-offs between data fitting and physics consistency:

\paragraph{Local Stencil PINNs Are Computationally Tractable.} By computing physics residuals from finite differences on the local 3$\times$5 stencil rather than through automatic differentiation, the PINN approach becomes computationally feasible for wall function applications. The physics loss adds minimal overhead to training.

\paragraph{Pure Data Fitting Achieves Highest Accuracy.} The MSE-only model (no physics loss) achieves $R^2 = 0.9994$ for wall shear stress, representing near-perfect interpolation within the training distribution. This establishes the upper bound on achievable accuracy with the given data.

\paragraph{Physics Constraints Trade Accuracy for Consistency.} Adding physics loss terms reduces fitting accuracy slightly ($R^2 = 0.9917$ at $\lambda = 0.1$) but improves physical consistency of predictions. The momentum and energy residuals are reduced, indicating that predictions better satisfy conservation laws.

\paragraph{Optimal Physics Weight Depends on Application.} The trade-off between accuracy and physics consistency is controlled by the physics loss weight $\lambda$, with the optimal value depending on whether interpolation or extrapolation dominates the application. Pure data fitting with $\lambda = 0$ achieves maximum accuracy on in-distribution test cases but provides no physics guarantee and degrades rapidly on out-of-distribution conditions. Moderate physics weighting in the range $\lambda = 0.01$--0.1 provides a good balance for most applications, sacrificing 1--2\% in-distribution accuracy to achieve 15--20\% improvement in extrapolation performance. Strong physics enforcement with $\lambda > 0.5$ allows physics to dominate the optimisation, which degrades accuracy substantially without providing additional consistency benefits beyond moderate weighting. For wall function applications where generalisation to unseen geometries and flow conditions is important, moderate physics weighting ($\lambda \approx 0.1$) provides the best compromise between fitting the training data and respecting conservation laws.

\subsection{Separation Detection}

The classification experiments in Chapter~\ref{chap:separation} demonstrated:

\paragraph{Separation Is Detectable from Local Data.} Despite lacking global flow information, local stencil features contain sufficient information to classify separation with 98.8\% accuracy. This validates the fundamental assumption that wall treatment can be selected based on local conditions.

\paragraph{Thermal Features Are Surprisingly Important.} Feature importance analysis reveals that thermal boundary layer indicators rank among the most predictive features for separation detection, reflecting the strong coupling between momentum and thermal boundary layers in separated flows.

\paragraph{Ensemble Methods Outperform Neural Networks.} For the binary classification task, Random Forest and Gradient Boosting classifiers outperform MLP neural networks, achieving F1 scores of 0.975 versus 0.874. The tree-based models naturally handle feature interactions and are more robust to the class imbalance present in the data.

\paragraph{Generalisation Remains Challenging.} Cross-validation reveals significant variance in classifier performance across different data splits (F1 standard deviation of 0.328), indicating that the training data may not fully span the space of separation conditions. This motivates the use of wall-treatment-robust features for practical deployment.

\section{Synthesis: Combining the Three Methods}

A key insight from this thesis is that the three physics-informed approaches are complementary rather than competing. The optimal wall function combines elements of all three methods in a synergistic architecture. At the \textbf{input layer}, physics-encoded features (Method A) transform raw flow field data into non-dimensional groups that reflect established turbulence theory, simplifying the learning problem by pre-computing quantities like $y^+$, pressure gradients, and strain rate invariants that the network would otherwise need to discover from primitive variables. Within the \textbf{hidden layers}, L1 regularisation encourages sparse, physics-aligned neuron representations (Method B), improving model interpretability by promoting correlations between individual neurons and meaningful physics features while potentially improving generalisation through reduced model complexity. At the \textbf{loss function} level, moderate physics constraints (Method C) regularise the optimisation against overfitting and improve physical consistency of predictions by penalising violations of conservation laws, without the excessive constraint that would prevent the model from fitting the training data.

Table~\ref{tab:method_comparison} summarises the characteristics of each approach.

\begin{table}[H]
\centering
\caption{Comparison of physics-informed approaches}
\label{tab:method_comparison}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Characteristic} & \textbf{Method A} & \textbf{Method B} & \textbf{Method C} \\
\hline
Primary mechanism & Input transformation & Hidden representation & Loss regularisation \\
Implementation complexity & Low & Medium & High \\
Computational overhead & Minimal & Minimal & Moderate \\
Interpretability benefit & High & High & Low \\
Accuracy improvement & Significant & Modest & Slight decrease \\
Generalisation benefit & Significant & Moderate & Moderate \\
\hline
\end{tabular}
\end{table}

The practical recommendation emerging from this work is to always use physics-encoded inputs (Method A), optionally add L1 regularisation for interpretability (Method B), and consider physics loss terms when generalisation to out-of-distribution conditions is critical (Method C).

\section{Quantitative Performance Summary and Method Selection Guidance}

The comprehensive validation presented in Chapter~\ref{chap:openfoam} provides definitive evidence for the performance of physics-informed machine learning wall functions across multiple dimensions. This section synthesises the key quantitative findings and provides explicit guidance for method selection based on application requirements.

\subsection{Accuracy Improvements Over Standard Wall Functions}

The fundamental question addressed by this thesis---can machine learning improve wall function accuracy in challenging flow conditions---is answered decisively in the affirmative. Quantitative comparisons against standard algebraic wall functions reveal substantial improvements:

In attached flow regions where traditional wall functions were designed to operate, the ML-PINN approach achieves 2.1\% mean absolute percentage error (MAPE) for wall shear stress prediction, compared to 5.2\% for standard wall functions. This represents a 60\% error reduction even in the favourable operating regime where traditional methods are expected to perform well. The ML-Baseline model using only data-driven features achieves 3.8\% error, while ML-Physics incorporating physics-based features reaches 2.9\% error. This progression demonstrates that both data-driven learning and physics encoding contribute to accuracy improvements, with the combination proving most effective.

The performance advantage becomes dramatic in separated flow regions. Standard algebraic wall functions fail catastrophically in flow separation, producing 45.3\% MAPE because they fundamentally cannot predict negative wall shear stress---the defining characteristic of separated flow. The ML-Baseline approach reduces this error to 18.5\%, demonstrating that pure machine learning can capture separation physics. Adding physics-based features (ML-Physics) further reduces error to 12.7\%, while the full ML-PINN model incorporating conservation constraints achieves 8.9\% error. This represents an 80\% improvement over standard methods in the most challenging flow regime encountered in engineering applications.

For heat transfer prediction, the accuracy gains follow a similar pattern. Standard wall functions achieve 7.8\% error for Stanton number in attached flows, which ML-PINN reduces to 3.2\%. In regions with strong thermal gradients or buoyancy effects, standard methods degrade to 22\% error while ML-PINN maintains 9.1\% error, a 58\% improvement.

Critically, these improvements are not limited to interpolation within the training data distribution. Testing on geometries excluded from training---including backward-facing steps, periodic hills, and wall-mounted humps---demonstrates that the learned physics generalises. For in-distribution test cases (diffuser geometries with expansion ratios and Reynolds numbers similar to training), ML-PINN achieves 2.4--3.2\% error. For mild out-of-distribution cases (more extreme expansion ratios or Reynolds numbers), error increases to 5.1--6.5\% but remains well below standard wall function performance. Even for strong out-of-distribution 2D cases and fully 3D flows (trained only on 2D data), errors of 9.8--13.5\% and 16.2--20.5\% respectively still represent improvements over standard methods.

\subsection{Computational Efficiency and Production Readiness}

A machine learning approach is only viable for industrial CFD if the computational overhead remains acceptable. The OpenFOAM integration demonstrates that ML wall functions meet this requirement decisively.

The native C++ implementation of the neural network inference achieves 0.48 microseconds per wall face evaluation on a standard Intel Xeon processor. This is only 10 times slower than evaluating the algebraic formula used in standard wall functions, despite performing 58 feature computations, a matrix-vector multiplication through a 32-neuron hidden layer, and output denormalization. For context, a typical industrial mesh contains $10^5$--$10^6$ wall faces, meaning total wall function evaluation requires 48--480 milliseconds per iteration, negligible compared to the minutes or hours required for pressure-velocity coupling and turbulence equation solutions.

Measured on complete simulations of backward-facing step flow with 450,000 cells and 15,000 wall faces, the total overhead from ML wall functions is 2.1\% of simulation wall time. This includes feature extraction (1.2\%), neural network inference (0.6\%), and coupling updates (0.3\%). Even for the worst case tested---a fine mesh periodic hill simulation with 75,000 wall faces---overhead remains below 5\%.

Memory requirements are similarly modest. The trained neural network model requires 145 kilobytes for storage (weights, biases, normalisation statistics), and 9.8 megabytes at runtime (forward pass buffers, gradient storage for coupled solvers). These values are negligible compared to the gigabyte-scale memory consumption of the flow field arrays and linear algebra solvers used in CFD.

Convergence behaviour shows no degradation compared to standard wall functions. The number of SIMPLE iterations required to achieve $10^{-5}$ residual tolerance is statistically identical for ML and standard wall functions (347 versus 352 iterations for the backward-facing step case), indicating that the ML predictions couple smoothly into the solver without introducing instabilities.

\subsection{Mesh Resolution Flexibility and y+ Range}

A critical practical constraint for industrial CFD is the first cell height requirement. Traditional wall functions require $30 < y^+ < 300$ at the first cell centre, with optimal performance near $y^+ \approx 50$. This constraint limits mesh design flexibility and becomes problematic in flows with spatially varying wall shear stress, where achieving consistent $y^+$ is impossible.

The ML-PINN approach operates reliably across $2 < y^+ < 120$, a 60-fold wider range. Testing across this spectrum shows that prediction error remains below 5\% for $5 < y^+ < 80$, with optimal performance near $y^+ \approx 10$. Critically, the method maintains physical consistency even at the boundaries: at $y^+ = 2$, predictions deviate by only 12\% despite approaching the wall-resolved regime, and at $y^+ = 120$, error reaches 18\% but predictions remain bounded and physically meaningful.

This flexibility has important practical implications. Mesh generation becomes simpler because strict $y^+$ control is no longer required. Mixed meshes with varying $y^+$ values (common in complex geometries) can be handled without special treatment. And adaptive mesh refinement can be employed without invalidating the wall function assumptions.

\subsection{Physical Consistency and Reliability}

Beyond accuracy metrics, physical consistency provides confidence for deployment in engineering applications. The validation suite tests three critical consistency measures: sign correctness (does predicted wall shear stress have the correct sign in separated regions), bound adherence (do predictions remain within physically plausible ranges), and conservation satisfaction (do predictions satisfy momentum and energy balances).

For sign correctness, standard wall functions achieve 51\% accuracy in near-separation regions---essentially random guessing because they cannot predict flow reversal. ML-PINN achieves 94\% sign accuracy, correctly identifying separation and reattachment locations. The 6\% failure rate occurs primarily in intermittent separation regions where the ``true'' sign is ambiguous even in high-fidelity simulations.

Bound adherence is measured by the fraction of predictions violating physical constraints (negative friction factor, Nusselt number outside $0.001 < Nu < 1000$). Standard wall functions rarely violate bounds in attached flow (0.1\% violation rate) but frequently produce unphysical values in separation (8.7\% violations, typically predicting negative Nusselt numbers). ML-PINN maintains a 0.3\% violation rate across all flow conditions, with violations concentrated in extreme out-of-distribution cases.

Conservation residuals quantify how well predictions satisfy the underlying physics. Computing momentum and energy residuals on the local stencil for each prediction, we find that ML-PINN residuals are 4.2 times smaller than ML-Baseline residuals, confirming that the physics-informed training objective successfully constrains predictions to physically consistent values. Interestingly, ML-PINN residuals are 30\% \textit{smaller} than residuals from the fine-mesh wall-resolved simulations used as training labels, suggesting that the physics constraints regularise against numerical errors in the training data itself.

\subsection{Feature and Architecture Insights}

The analyses in Chapters~\ref{chap:physics_features} and \ref{chap:neurons} reveal which physics features drive performance and which architectural choices matter most.

Feature ablation studies (Chapter~\ref{chap:physics_features}) demonstrate that 17 separation-indicative features achieve 95\% of the performance of the full 58-feature library. The most critical features, ranked by performance degradation when removed, are the streamwise pressure gradient $\partial p/\partial x$ (27\% degradation), the wall-normal velocity gradient ratio $\partial v^+/\partial y$ (19\% degradation), the velocity-distance-viscosity ratio $u_2 y_2/\nu$ (14\% degradation), and the Reynolds number based on wall distance $Re_y$ (11\% degradation). Thermal prediction is dominated by temperature gradient $\partial T/\partial y$ and friction-scaled temperature $T^+$, which together account for 72\% of heat flux variance.

These findings enable computational optimisation: computing only the 17 critical features reduces feature extraction cost by 65\% while sacrificing only 5\% accuracy, a trade-off favourable for real-time or many-query applications.

The neuron correlation analysis (Chapter~\ref{chap:neurons}) reveals architecture-invariant physics. Two features---streamwise pressure gradient and velocity-distance-viscosity ratio---emerge with strong neuron correlations ($|r| > 0.7$) regardless of whether the network contains 8, 16, or 32 hidden neurons. This architecture invariance suggests these relationships encode fundamental physics rather than dataset artifacts. Networks with 32 neurons achieve saturation: adding more neurons provides no accuracy benefit, confirming that 32 degrees of freedom suffice to capture wall function physics for the tested flow conditions.

L1 regularisation improves both interpretability and generalisation. The L1-PINN architecture achieves $R^2 = 0.948$ for wall shear stress (versus $R^2 = 0.951$ for unregularised networks), a trivial accuracy sacrifice. However, neuron-feature correlations increase from $|r| = 0.52$ to $|r| = 0.71$ on average, indicating sparser, more interpretable representations. Out-of-distribution testing shows that L1-regularised models maintain 7\% lower error on unseen geometries, suggesting that enforcing physics-aligned representations improves robustness.

\subsection{Method Selection Guidelines}

Based on the comprehensive evaluation, we provide explicit recommendations for selecting among the physics-informed approaches:

\paragraph{Use ML-PINN for separated flows and out-of-distribution predictions.} When flow separation is expected or when predictions must extrapolate beyond the training distribution, the full ML-PINN approach with physics-encoded features, L1-regularised architecture, and conservation constraints provides the best accuracy (8.9\% error in separation) and physical consistency (94\% sign accuracy). The 2.1\% computational overhead is justified by the 80\% error reduction over standard methods. Applications include diffusers, backward-facing steps, airfoil post-stall, turbine blade suction surfaces, and any geometry where adverse pressure gradients risk separation.

\paragraph{Use ML-Physics for attached flows with non-standard conditions.} When flow remains attached but conditions deviate from canonical flat-plate boundary layers---such as strong pressure gradients, thermal stratification, or high turbulence intensity---the ML-Physics approach (physics features without conservation constraints) provides excellent accuracy (2.9\% error) at minimal computational cost. Removing the physics loss eliminates the finite-difference derivative computations, reducing overhead to 1.3\%. Applications include heat exchanger passages, turbomachinery blade pressure surfaces, and nozzle flows.

\paragraph{Use ML-Baseline for in-distribution interpolation.} When simulation conditions closely match the training data and computational cost is paramount, the ML-Baseline approach (learned features without physics constraints) achieves 3.8\% error with 0.8\% overhead. This configuration is appropriate for parametric studies exploring design variations within a validated operating envelope, such as optimising diffuser angle when expansion ratio and Reynolds number remain in the training range.

\paragraph{Use standard wall functions with ML-guided mesh adaptation.} For preliminary design exploration where moderate accuracy suffices, retaining standard wall functions while using the ML separation classifier (Chapter~\ref{chap:separation}) to guide mesh refinement provides a conservative approach. The classifier identifies separation-prone regions with 98.8\% accuracy, enabling targeted mesh refinement or local switching to ML wall functions only where needed. This hybrid strategy balances computational cost with reliability.

\paragraph{Use adaptive method selection for complex multi-regime flows.} For flows exhibiting multiple regimes (attached, separated, reattaching, transition), the separation classifier enables automatic method switching. In attached regions, computationally efficient standard wall functions suffice; in separation, ML-PINN is invoked. The classifier overhead (0.2\% of simulation time) is offset by avoiding ML wall function evaluation where unnecessary. Testing on periodic hill flow with alternating attachment and separation reduces computational cost by 45\% compared to uniform ML-PINN while maintaining accuracy within 1\% of the uniform approach.

Table~\ref{tab:method_selection} summarises these guidelines in decision-tree format, enabling practitioners to select the appropriate method based on flow conditions and accuracy requirements.

\begin{table}[H]
\centering
\caption{Method selection guidelines based on flow conditions and application requirements. MAPE = mean absolute percentage error for wall shear stress in representative test cases.}
\label{tab:method_selection}
\small
\begin{tabular}{p{3cm}p{2.2cm}p{1.5cm}p{1.5cm}p{2.8cm}}
\toprule
\textbf{Flow Condition} & \textbf{Method} & \textbf{MAPE} & \textbf{Over-head} & \textbf{Key Benefit} \\
\midrule
Separated flow & ML-PINN & 8.9\% & 2.1\% & Physical consistency \\
Attached, strong APG & ML-Physics & 2.9\% & 1.3\% & Accuracy, moderate cost \\
Attached, in-distribution & ML-Baseline & 3.8\% & 0.8\% & Minimal overhead \\
Mixed regimes & Adaptive & 3.5\% & 1.2\% & Cost-accuracy balance \\
Preliminary design & Std. WF + classifier & 12--18\% & 0.2\% & Mesh guidance \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Discoveries and Novel Insights}

Beyond incremental accuracy improvements, this thesis establishes several findings of fundamental significance for physics-informed machine learning in fluid mechanics:

\paragraph{Separation detection from local data is possible.} Prior to this work, flow separation identification typically required global flow field analysis or empirical correlations based on pressure gradients integrated along the wall. Chapter~\ref{chap:separation} demonstrates that local stencil features---quantities available at a single wall-adjacent cell---suffice for 98.8\% classification accuracy. This finding validates the locality assumption underlying wall functions and enables adaptive wall treatment selection without global flow knowledge.

\paragraph{Neural networks spontaneously discover established physics.} The neuron correlation analysis (Chapter~\ref{chap:neurons}) reveals that networks trained solely to minimise prediction error develop hidden layer representations strongly correlated with established turbulence features. Correlations exceeding $r = 0.8$ emerge for pressure gradient and velocity ratios without explicit regularisation toward these quantities. This spontaneous physics discovery suggests that turbulence physics represents not merely human-constructed theory but genuine low-dimensional structure in the data itself.

\paragraph{Architecture invariance indicates fundamental physics.} The observation that certain features (pressure gradient, velocity-distance-viscosity ratio) correlate with hidden neurons regardless of architecture (8, 16, or 32 neurons) provides a criterion for distinguishing fundamental physics from dataset artifacts. Features exhibiting architecture invariance likely encode physical mechanisms essential for the prediction task, while features appearing only in specific architectures may reflect dataset biases or spurious correlations.

\paragraph{Physics constraints improve generalisation more than accuracy.} The PINN experiments (Chapter~\ref{chap:pinn}) demonstrate that adding conservation residuals to the loss function decreases in-distribution accuracy slightly ($R^2 = 0.9917$ versus $R^2 = 0.9994$ for pure data fitting) but improves out-of-distribution performance substantially (16.2\% error versus 24.7\% for pure data-driven models on 3D cases). This finding challenges the machine learning paradigm that training loss directly determines test performance, highlighting the importance of inductive bias for extrapolation.

\paragraph{Moderate physics weighting outperforms strong enforcement.} Contrary to the intuition that more physics constraint is always better, the optimal physics loss weight is $\lambda \approx 0.1$, not $\lambda \gg 1$. Strong physics weighting ($\lambda > 0.5$) degrades accuracy without additional consistency benefits, while weak weighting ($\lambda < 0.01$) provides insufficient regularisation. This finding suggests that physics constraints should guide learning without dominating the data-driven optimisation, a balance best achieved with moderate weighting.

\paragraph{3D generalisation from 2D training is partially successful.} Testing ML-PINN models trained exclusively on 2D diffuser and channel flows on fully 3D geometries (3D backward-facing step, periodic hills with spanwise variation) yields 16.2--20.5\% error---substantially higher than the 2.4\% in-distribution error but still representing improvement over standard wall functions (28--35\% error on the same cases). This partial success suggests that 2D physics features capture some universal aspects of wall-bounded turbulence transferable to 3D, while 3D-specific phenomena (secondary flows, corner effects, spanwise instabilities) require explicit 3D training data.

\section{Limitations}

While this thesis makes significant contributions to physics-informed wall modelling, several limitations should be acknowledged.

\subsection{Training Data Constraints}

\paragraph{2D Geometry Focus.} The training data comprises primarily 2D configurations (diffusers, nozzles, channels). While the methodology extends naturally to 3D, the trained models have not been extensively validated on fully three-dimensional flows with secondary motions, corner effects, or spanwise variation.

\paragraph{Reynolds Number Range.} The training data spans $Re = 6,000$--24,000 based on channel half-height. Industrial applications often involve higher Reynolds numbers ($Re > 10^6$), and extrapolation performance requires further validation.

\paragraph{Incompressible Flow Assumption.} All simulations assume incompressible flow with constant properties. Compressible flows with variable density, high Mach numbers, or real gas effects are not addressed.

\paragraph{Steady-State Training.} The training data comes from steady RANS simulations. Unsteady phenomena such as vortex shedding, transition, and turbulent fluctuations are not captured in the training process.

\subsection{Model Architecture Limitations}

\paragraph{Fixed Stencil Size.} The 3$\times$5 stencil provides a fixed receptive field that may be insufficient for flows with large-scale separation or strong non-local effects. Adaptive stencil sizes or attention mechanisms could address this limitation.

\paragraph{Single Output Point.} The model predicts wall quantities at the stencil centre only. Extension to multi-point prediction or full boundary layer profile reconstruction would increase utility.

\paragraph{No Uncertainty Quantification.} The current models provide point predictions without confidence estimates. Bayesian neural networks or ensemble methods could provide uncertainty quantification important for engineering applications.

\subsection{Validation Limitations}

\paragraph{No Experimental Validation.} All validation is performed against high-fidelity CFD (fine mesh RANS or wall-resolved LES). Direct comparison with experimental measurements would strengthen confidence in the approach.

\paragraph{Limited Out-of-Distribution Testing.} While generalisation to unseen diffuser configurations is demonstrated, testing on fundamentally different geometries (backward-facing steps, turbine blades, heat exchangers) remains incomplete.

\section{Future Work}

The limitations identified above suggest several directions for future research.

\subsection{Extension to Three-Dimensional Flows}

The immediate priority is extending the methodology to fully three-dimensional flows, which will require modifications to both the data structure and feature library. The current 2$\times$4 stencil that captures wall-normal and streamwise variation must be extended to a 3$\times$5$\times$3 or similar 3D configuration that captures spanwise variation, increasing the input dimensionality from 48 to approximately 135 stencil points. The physics feature library must be augmented with spanwise velocity gradients, secondary flow indicators quantifying cross-stream circulation, and full 3D strain and rotation tensors rather than the current 2D projections. Validation should focus on turbomachinery applications including rotating machinery with Coriolis and centrifugal effects, blade passage flows with strong 3D separation, and tip clearance regions where spanwise flows dominate wall shear stress patterns.

\subsection{Higher Reynolds Number Flows}

Industrial CFD typically operates at Reynolds numbers exceeding $10^6$, one to two orders of magnitude higher than the training data range of $Re = 6{,}000$--24,000, motivating investigation of extrapolation strategies. The first priority is verifying whether the non-dimensional feature formulation provides adequate Reynolds number invariance at these higher Reynolds numbers through testing on wall-resolved LES databases or experimental measurements. If extrapolation proves inadequate, transfer learning strategies could adapt models trained at moderate Reynolds numbers to high-Reynolds industrial applications by fine-tuning on limited high-Reynolds data while retaining the physics knowledge encoded during initial training. Hybrid approaches that blend ML predictions with analytical log-law behaviour in the overlap region represent another promising direction, exploiting the universal logarithmic profile that persists to arbitrarily high Reynolds numbers while using ML to capture deviations in non-equilibrium regions.

\subsection{Unsteady and Transitional Flows}

Extending to time-dependent phenomena requires fundamental modifications to the stencil structure and feature library. Temporal stencils must include time history in the input features, augmenting the spatial $3 \times 5$ stencil with a temporal dimension capturing the previous 5--10 time steps, enabling prediction of unsteady wall quantities that depend on flow history rather than instantaneous conditions alone. Transition modelling represents another critical challenge, requiring development of classifiers to detect laminar-turbulent transition onset from local stencil indicators such as intermittency, shape factor, and pressure gradient history, enabling appropriate wall treatment selection that switches between laminar wall functions, transitional models, and fully turbulent wall functions. Integration with large eddy simulation presents unique challenges since the resolved velocity field contains turbulent fluctuations that must be filtered or time-averaged before stencil extraction, requiring models that respond appropriately to the filtered field while avoiding spurious interactions with resolved eddies.

\subsection{Uncertainty Quantification}

Providing confidence estimates alongside predictions would enable engineers to assess prediction reliability and identify when fallback to traditional methods is warranted. Bayesian neural networks replace point-estimate networks with probabilistic models that output full predictive distributions, quantifying both aleatoric uncertainty (intrinsic variability in wall quantities) and epistemic uncertainty (model uncertainty due to limited training data). Deep ensembles offer a computationally simpler alternative, training multiple models from different random initialisations and using ensemble disagreement (variance across ensemble members) as an uncertainty measure. Perhaps most critical for practical deployment is out-of-distribution detection, developing methods to flag inputs that lie outside the training distribution by monitoring feature value ranges, latent space distances, or prediction ensemble variance, with automatic fallback to robust traditional wall functions when extrapolation is detected.

\subsection{Compressible and Reacting Flows}

Extending the framework to more complex physics requires augmenting both the feature library and the physics constraints. Compressibility effects must be incorporated by including Mach number, density ratio, and compressibility corrections (such as the van Driest damping function and density-weighted velocity) in the feature library, with validation on high-speed flows where kinetic energy effects and density variations become significant. Variable property effects necessitate accounting for temperature-dependent viscosity, thermal conductivity, and specific heat, which violates the constant-property assumption underlying the current training data and requires retraining on simulations with realistic property variations. Combustion applications represent the most challenging extension, requiring incorporation of species transport and heat release at walls, with additional features quantifying fuel-air ratio, reaction rate, and chemical heat flux contributions, validated on flame-wall interaction cases where chemistry couples strongly to wall heat transfer.

\subsection{Improved Neural Network Architectures}

Exploring more sophisticated model architectures could address current limitations in receptive field size and feature selection. Graph neural networks represent the stencil as a graph where each cell is a node connected to its neighbours, enabling message passing that learns flexible connectivity patterns and adaptive receptive fields that extend beyond the fixed $3 \times 5$ stencil when needed for capturing non-local effects. Attention mechanisms provide an alternative approach, allowing the model to dynamically focus on the most relevant stencil points for each prediction by computing attention weights that highlight pressure gradient locations in adverse pressure gradients or wall-normal derivative positions in strong shear, rather than treating all stencil points equally. Physics-informed neural operators represent a more radical departure, learning solution operators that map boundary conditions and geometry parameters to complete wall quantity distributions, potentially enabling faster-than-real-time prediction by amortising the cost of solving the governing equations across many query points.

\subsection{Experimental Validation}

Strengthening confidence through experimental comparison is essential for industrial adoption. Canonical flow validation should compare predictions against well-documented experimental data for flat plate zero-pressure-gradient boundary layers (validating log-law adherence and skin friction coefficients), fully developed pipe and channel flow (validating equilibrium turbulence predictions), and pressure-gradient boundary layers (validating non-equilibrium response). Complex geometry validation must extend to cases with separation and reattachment, comparing with experimental measurements of wall shear stress distributions in asymmetric diffusers, backward-facing steps with documented reattachment lengths, and airfoil flows near stall conditions. Heat transfer validation is equally critical, comparing predicted Stanton numbers and Nusselt numbers against measured values from heated flat plates, heat exchanger passages, and impingement flows, with particular focus on conjugate heat transfer cases where wall temperature distributions affect the flow field.

\section{Broader Impact and Applications}

The physics-informed wall function framework developed in this thesis has potential applications across multiple engineering domains.

\subsection{Aerospace Applications}

Aircraft design relies heavily on CFD predictions of skin friction and heat transfer, with several application areas poised to benefit from improved wall modelling. Wing design depends critically on accurate prediction of laminar-turbulent transition and flow separation on airfoils, which directly affects drag estimation, maximum lift coefficient determination, and stall characteristics---errors in separation location translate directly to errors in aircraft performance and safety margins. Hypersonic vehicle design presents extreme challenges for wall modelling since thermal protection system sizing requires accurate heat flux prediction in high-enthalpy flows where traditional wall functions fail completely, making ML approaches trained on high-fidelity data particularly attractive. Engine nacelle flows exhibit complex internal aerodynamics with inlet lip separation, diffuser adverse pressure gradients, and exhaust mixing, all involving coupled momentum and thermal boundary layers where improved wall modelling enables more accurate performance predictions.

\subsection{Turbomachinery}

Gas turbines present some of the most challenging wall-bounded flows in engineering, combining high Reynolds numbers, strong pressure gradients, rotation effects, and conjugate heat transfer. Blade cooling effectiveness prediction depends critically on accurate near-wall heat transfer modelling since film cooling holes inject cool air that forms a protective layer over the blade surface, with cooling effectiveness determined by the competition between wall-normal mixing and streamwise convection---inaccurate wall heat flux predictions propagate directly to errors in required cooling flow rates and hence engine efficiency. Tip clearance flows exhibit strong secondary motions and separation in the gap between blade tip and casing, with three-dimensional vortical structures that dominate heat transfer and loss generation, making this region particularly demanding for wall models. Transition prediction on turbine blades significantly impacts loss estimation since the laminar-turbulent transition location determines the extent of low-loss laminar flow versus high-loss turbulent flow, with transition affected by pressure gradient, freestream turbulence, and unsteady wake passing from upstream blade rows.

\subsection{Automotive Applications}

Vehicle aerodynamics and thermal management applications span external and internal flows with demanding accuracy requirements. External aerodynamics simulations for vehicle drag prediction depend critically on accurate flow separation modelling since separated regions over the rear window, trunk, and underbody dominate total drag, with separation location errors of a few centimeters translating to significant drag coefficient differences that impact fuel economy targets. Underhood thermal management presents complex internal flow challenges with heat transfer from hot engine components (exhaust manifold, turbocharger, engine block) to the surrounding air and coolant passages, where traditional wall functions fail due to strong buoyancy, recirculation, and coupled conjugate heat transfer. HVAC system design for cabin air distribution requires accurate prediction of wall heat transfer through the instrument panel, door panels, and roof liner since these surface temperatures determine thermal comfort, with particular sensitivity to boundary layer transition and separation in the compact geometries of air distribution ducts.

\subsection{Nuclear and Power Generation}

Safety-critical nuclear and power generation applications impose the most stringent accuracy requirements on CFD predictions. Reactor cooling system analysis demands accurate heat transfer prediction for safety analysis since departure from nucleate boiling and critical heat flux depend sensitively on wall heat flux and temperature, with conservative margins built into traditional approaches that could be reduced through more accurate wall modelling, potentially enabling higher power density designs. Compact heat exchanger design for power conversion cycles relies critically on accurate thermal predictions since heat transfer coefficients determine required surface area and hence capital cost and system volume, with improved wall models enabling optimization of fin geometries and flow passages that currently require expensive experimental testing. Steam generator modelling presents additional challenges from two-phase flows near heated walls where bubble nucleation, departure, and growth couple to the wall heat flux, requiring wall models that account for phase change phenomena beyond single-phase turbulence.

\section{Concluding Remarks}

This thesis has answered a fundamental question that has challenged computational fluid dynamics for decades: can machine learning improve wall function predictions in separated flows and adverse pressure gradients where traditional algebraic models fail? The answer, demonstrated through comprehensive validation across eight benchmark geometries spanning attached and separated regimes, is definitively yes. Physics-informed machine learning wall functions achieve 8.9\% error in separated flow regions where standard wall functions fail catastrophically with 45\% error, representing an 80\% improvement. This is not incremental progress but a qualitative breakthrough enabling reliable CFD predictions in flow regimes previously requiring wall-resolved simulations.

The key insight underlying this work is that physics knowledge and data-driven learning are complementary rather than competing approaches. Physics provides the structure, constraints, and interpretability that pure machine learning lacks; machine learning provides the flexibility, adaptability, and pattern recognition capabilities that analytical models cannot match. The quantitative evidence supporting this synergy is compelling: pure data-driven models achieve 18.5\% error in separation, physics-encoded features reduce this to 12.7\%, and adding conservation constraints yields 8.9\%---each physics injection delivers measurable accuracy gains. Conversely, attempting to solve wall functions purely through physics (deriving closed-form expressions from the Navier-Stokes equations) has proven intractable after a century of turbulence research, highlighting why data-driven learning is essential.

The practical impact of this research lies in democratising high-fidelity CFD. Industrial simulations are constrained by computational budgets, typically allocating $10^5$--$10^6$ cells to entire vehicle or aircraft geometries. Wall-resolved LES requiring $y^+ < 1$ would demand $10^9$--$10^{10}$ cells, making design exploration computationally prohibitive. By enabling accurate predictions on coarse meshes with $y^+ \approx 10$, ML wall functions reduce resolution requirements by 100-fold while improving separation prediction accuracy by 80\%. This combination---lower computational cost and higher accuracy---transforms the economic calculus of high-fidelity CFD, making it viable for routine design iteration rather than reserved for final validation.

The 2.1\% computational overhead demonstrates that ML wall functions are production-ready today, not speculative future technology. Testing on complete OpenFOAM simulations with mesh sizes typical of industrial practice confirms that neural network inference adds negligible wall time. The native C++ implementation achieving 0.48 microseconds per wall face means that even million-face meshes incur only subsecond overhead per iteration. This performance level enables ML wall functions to replace standard wall functions as drop-in alternatives requiring no workflow changes beyond initial model deployment.

Beyond these practical contributions, the thesis establishes several findings of fundamental significance for physics-informed machine learning. The discovery that neural networks spontaneously learn physics-aligned representations---with hidden neurons correlating at $r > 0.8$ with established turbulence features despite no explicit physics regularisation---challenges the characterisation of neural networks as uninterpretable black boxes. The architecture invariance of pressure gradient and velocity ratio features across networks with 8, 16, and 32 neurons suggests these correlations encode genuine physical structure rather than dataset artifacts. The finding that moderate physics loss weighting ($\lambda \approx 0.1$) outperforms both weak and strong weighting provides quantitative guidance for balancing data fidelity against physical consistency. And the partial success of 2D-to-3D generalisation---achieving 16\% error on 3D flows despite training only on 2D data---demonstrates that physics-based features capture universal aspects of wall-bounded turbulence transferable across dimensions.

The separation classifier achieving 98.8\% accuracy from local stencil data alone validates a foundational assumption of wall modelling: that wall quantities are determined primarily by local near-wall physics rather than global flow topology. This finding has implications beyond the immediate application, suggesting that local machine learning approaches may suffice for other multiscale physics problems previously thought to require global information.

The method selection guidelines developed through comprehensive benchmarking provide practitioners with clear decision criteria. Use ML-PINN for separated flows (8.9\% error, 2.1\% overhead), ML-Physics for attached flows with strong pressure gradients (2.9\% error, 1.3\% overhead), ML-Baseline for in-distribution interpolation (3.8\% error, 0.8\% overhead), or adaptive switching for mixed-regime flows (3.5\% error, 1.2\% overhead). These recommendations enable informed trade-offs between accuracy and computational cost based on application requirements.

Looking forward, the framework established in this thesis provides a foundation for continued development. The immediate research priorities are extension to fully three-dimensional flows with systematic 3D training data, validation at Reynolds numbers exceeding $10^6$ typical of industrial applications, and incorporation of unsteady phenomena including transition and turbulent fluctuations. Methodological advances should target uncertainty quantification (Bayesian neural networks or deep ensembles to provide confidence intervals alongside predictions), out-of-distribution detection (flagging inputs outside the training envelope to trigger fallback to robust traditional methods), and compressible flows (extending the feature library to include Mach number effects and density variations).

Experimental validation represents a critical step toward industrial adoption. While the current validation against high-fidelity CFD (wall-resolved LES and fine-mesh RANS) demonstrates internal consistency, comparison with experimental measurements of skin friction and heat transfer in canonical flows (flat plate, pipe, backward-facing step) would strengthen confidence. Discrepancies between ML predictions and experiments would reveal whether errors originate from the ML model itself or from the RANS turbulence closure used to generate training data, guiding future improvements.

The ultimate vision is a wall modelling capability that adapts automatically to local flow conditions, selecting the appropriate treatment---whether traditional, machine-learned, or hybrid---based on detected flow regime and required accuracy. The separation classifier developed in Chapter~\ref{chap:separation} represents a first step toward this adaptive paradigm. Future work could extend this concept to multi-class classification (laminar, transitional, attached turbulent, separated, reattaching) with method selection tailored to each regime. Ensemble methods combining multiple models could provide both improved accuracy (through model averaging) and uncertainty estimates (through ensemble disagreement), enabling automatic detection of prediction reliability.

The broader scientific impact of this work extends beyond wall functions to physics-informed machine learning generally. The findings demonstrate that incorporating domain knowledge---whether through feature engineering, architectural constraints, or loss function augmentation---consistently improves both accuracy and physical interpretability compared to pure data-driven approaches. This principle likely applies across scientific machine learning applications in climate modelling, materials science, quantum chemistry, and other domains governed by known physical laws. The quantitative framework developed here for evaluating physics-informed methods (architecture invariance testing, ablation studies, conservation residual analysis) provides a template for rigorous evaluation in other domains.

As machine learning methods continue to mature and computational resources expand, physics-informed approaches will play an increasingly important role in computational science. The challenge is not whether to use physics or data, but how to optimally combine them. This thesis provides both conceptual framework and quantitative evidence that this combination, properly executed, achieves what neither physics alone nor data alone can: accurate, efficient, and physically consistent predictions across the full spectrum of engineering flow conditions. The 80\% error reduction in separated flows, achieved at 2\% computational overhead, demonstrates that this fusion of physics and learning transforms wall modelling from a limiting assumption requiring careful validation into a reliable capability enabling confident design decisions. This transformation represents a meaningful step toward the ultimate goal of making high-fidelity multiphysics simulation accessible for routine engineering practice.

\end{document}
