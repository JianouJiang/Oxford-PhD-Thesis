% !TeX root = ThesisMain.tex
% !TeX program = XeLaTeX
% !TeX encoding = UTF-8
% !TeX spellcheck = en_GB

\documentclass[../ThesisMain]{subfiles}
\ifSubfilesClassLoaded{}{}%

\begin{document}
\doublespacing%
\chapter{Physics-Constrained Learning}\label{chap:pinn}

The preceding chapters developed wall function models through data-driven feature engineering (Chapter~\ref{chap:physics_features}) and discovered physics relationships through neuron correlation analysis (Chapter~\ref{chap:neurons}). This chapter takes a complementary approach: rather than hoping the network discovers physics implicitly, we encode conservation laws directly into the training objective \cite{raissi2019physics, 2511_14497_v1, 2503_17704_v1}. The resulting Physics-Informed Neural Network (PINN) framework constrains learning to solutions that satisfy---or approximately satisfy---the governing equations of fluid mechanics \cite{2205_08663_v2, 2409_19851_v1, 2301_00106_v2}.

\section{Physics-Informed Neural Networks for Wall Functions}
\label{sec:pinn_intro}

Traditional supervised learning minimizes the prediction error on training data without regard for physical consistency \cite{2206_05226_v2, 2005_09023_v2}. A network predicting wall shear stress $\tau_w$ from near-wall flow quantities might achieve low mean squared error while producing predictions that violate momentum conservation, energy balance, or mass continuity \cite{2505_00343_v1, 2406_00471_v1}. Such violations may be invisible in interpolation but become catastrophic during extrapolation to conditions outside the training distribution \cite{2307_13144_v1, 2409_04143_v1}.

Physics-Informed Neural Networks, introduced by Raissi et al. \cite{raissi2019physics}, address this limitation by augmenting the data loss with physics residuals:
\begin{equation}
    \mathcal{L} = \underbrace{\mathcal{L}_{\text{data}}}_{\text{MSE on labels}} + \lambda_{\text{physics}} \underbrace{\mathcal{L}_{\text{physics}}}_{\text{PDE residuals}}
    \label{eq:pinn_loss}
\end{equation}
where $\mathcal{L}_{\text{data}}$ measures agreement with training labels and $\mathcal{L}_{\text{physics}}$ penalizes violations of the governing equations evaluated at collocation points. The hyperparameter $\lambda_{\text{physics}}$ balances fitting accuracy against physical consistency.

Standard PINN implementations use automatic differentiation to compute PDE residuals throughout the computational domain, requiring network evaluations at thousands of collocation points per training step \cite{2511_14497_v1, 2105_10889_v1, 2404_03542_v1}. For wall function applications, this global approach is computationally prohibitive and physically inappropriate: we seek models that predict wall quantities from local near-wall information, not models that solve the entire flow field \cite{2309_15294_v2, 2309_02109_v1}.

This chapter develops a \textit{local stencil-based} PINN variant tailored for wall functions. Rather than enforcing conservation laws globally, we evaluate physics residuals on the same $2 \times 4$ stencil used for input features, constraining the network to produce predictions consistent with local conservation principles. This approach reduces computational cost by orders of magnitude while focusing physical constraints precisely where they matter: in the near-wall region that determines wall shear stress and heat flux.

\section{Conservation Laws as Soft Constraints}
\label{sec:conservation_laws}

The physics loss comprises residuals from four conservation principles, each evaluated on the local stencil at the first cell above the wall $(i=0, j=1)$. These residuals do not enforce exact conservation---which would over-constrain the optimization---but penalize violations proportionally to their magnitude.

\subsection{Streamwise Momentum Conservation}

The steady-state streamwise momentum equation for incompressible flow is:
\begin{equation}
    \rho \left( U_x \frac{\partial U_x}{\partial x} + U_y \frac{\partial U_x}{\partial y} \right) = -\frac{\partial p}{\partial x} + \mu \frac{\partial^2 U_x}{\partial y^2}
    \label{eq:momentum_x}
\end{equation}
where we have neglected streamwise diffusion under boundary layer assumptions. The residual measures the imbalance between convective acceleration, pressure gradient, and viscous stress:
\begin{equation}
    R_u = \rho \left( U_x \frac{\partial U_x}{\partial x} + U_y \frac{\partial U_x}{\partial y} \right) + \frac{\partial p}{\partial x} - \mu \frac{\partial^2 U_x}{\partial y^2}
    \label{eq:R_u}
\end{equation}

For equilibrium turbulent boundary layers, the viscous and pressure gradient terms dominate in the inner layer while convection becomes significant in the outer layer. The residual $R_u$ measures departure from this balance.

\subsection{Wall-Normal Momentum with Buoyancy}

The wall-normal momentum equation includes buoyancy through the Boussinesq approximation:
\begin{equation}
    \rho \left( U_x \frac{\partial U_y}{\partial x} + U_y \frac{\partial U_y}{\partial y} \right) = -\frac{\partial p}{\partial y} + \mu \frac{\partial^2 U_y}{\partial y^2} + \rho g \beta (T - T_{\text{ref}})
    \label{eq:momentum_y}
\end{equation}
where $\beta$ is the thermal expansion coefficient and $T_{\text{ref}}$ is the reference temperature. The residual becomes:
\begin{equation}
    R_v = \rho \left( U_x \frac{\partial U_y}{\partial x} + U_y \frac{\partial U_y}{\partial y} \right) + \frac{\partial p}{\partial y} - \mu \frac{\partial^2 U_y}{\partial y^2} - \rho g \beta (T - T_{\text{ref}})
    \label{eq:R_v}
\end{equation}

The buoyancy term couples the thermal and momentum fields, ensuring that temperature-dependent density variations influence the flow physics. For the isothermal cases dominating our training data, this term contributes minimally; for strongly heated walls, it becomes essential for physical consistency.

\subsection{Energy Conservation}

The steady-state energy equation for incompressible flow with constant properties is:
\begin{equation}
    \rho c_p \left( U_x \frac{\partial T}{\partial x} + U_y \frac{\partial T}{\partial y} \right) = k \frac{\partial^2 T}{\partial y^2}
    \label{eq:energy}
\end{equation}
where we have again neglected streamwise conduction. The residual measures the imbalance between convective heat transport and wall-normal conduction:
\begin{equation}
    R_T = \rho c_p \left( U_x \frac{\partial T}{\partial x} + U_y \frac{\partial T}{\partial y} \right) - k \frac{\partial^2 T}{\partial y^2}
    \label{eq:R_T}
\end{equation}

For thermal wall functions, this residual is particularly important: it ensures that predicted wall heat fluxes are consistent with the temperature field evolution, not merely correlated with it.

\subsection{Mass Conservation}

The incompressibility constraint requires zero velocity divergence:
\begin{equation}
    \frac{\partial U_x}{\partial x} + \frac{\partial U_y}{\partial y} = 0
    \label{eq:continuity}
\end{equation}
yielding the simplest residual:
\begin{equation}
    R_{\text{div}} = \frac{\partial U_x}{\partial x} + \frac{\partial U_y}{\partial y}
    \label{eq:R_div}
\end{equation}

Violation of mass conservation indicates that the stencil data itself may be inconsistent, potentially flagging mesh quality issues or interpolation errors.

\section{Stencil-Based Finite Difference Implementation}
\label{sec:finite_differences}

Unlike traditional PINNs that use automatic differentiation, our local approach computes derivatives using finite differences on the $2 \times 4$ stencil. This choice is deliberate: finite differences match the discretization used in the CFD solver, ensuring that the physics residuals measure conservation in the same sense that the underlying simulation enforces it.

\subsection{Derivative Approximations}

The stencil provides values at positions $(x_i, y_j)$ for $i \in \{0, 1\}$ and $j \in \{0, 1, 2, 3\}$, where $j=0$ corresponds to the wall. First derivatives use central differences where possible and one-sided differences at boundaries:
\begin{align}
    \frac{\partial \phi}{\partial x}\bigg|_{i,j} &\approx \frac{\phi_{i+1,j} - \phi_{i-1,j}}{2\Delta x} \quad \text{(central)} \label{eq:dphidx} \\
    \frac{\partial \phi}{\partial y}\bigg|_{i,j} &\approx \frac{\phi_{i,j+1} - \phi_{i,j-1}}{2\Delta y} \quad \text{(central)} \label{eq:dphidy}
\end{align}

At the wall boundary ($j=0$), wall-normal derivatives use one-sided differences:
\begin{equation}
    \frac{\partial \phi}{\partial y}\bigg|_{i,0} \approx \frac{-3\phi_{i,0} + 4\phi_{i,1} - \phi_{i,2}}{2\Delta y}
    \label{eq:dphidy_wall}
\end{equation}

Second derivatives for the diffusion terms use the standard three-point stencil:
\begin{equation}
    \frac{\partial^2 \phi}{\partial y^2}\bigg|_{i,j} \approx \frac{\phi_{i,j+1} - 2\phi_{i,j} + \phi_{i,j-1}}{\Delta y^2}
    \label{eq:d2phidy2}
\end{equation}

\subsection{Wall Boundary Residuals}

Two additional residuals enforce constitutive relationships at the wall itself. The wall shear stress residual compares the network prediction against Newton's law of viscosity:
\begin{equation}
    R_\tau = \left| \mu \frac{\partial U_x}{\partial y}\bigg|_{\text{wall}} - \tau_w^{\text{pred}} \right|
    \label{eq:R_tau}
\end{equation}

Similarly, the wall heat flux residual compares against Fourier's law:
\begin{equation}
    R_q = \left| -k \frac{\partial T}{\partial y}\bigg|_{\text{wall}} - q_w^{\text{pred}} \right|
    \label{eq:R_q}
\end{equation}

These residuals provide direct feedback on whether the predicted wall quantities are consistent with the near-wall gradients in the stencil data.

\subsection{Residual Normalization}

The six residuals have different physical dimensions and magnitudes. Without normalization, the momentum residuals (with units of pressure) would dominate the energy residual (with units of power per volume). We normalize each residual by characteristic scales derived from the stencil data:
\begin{align}
    \hat{R}_u &= R_u / (\tfrac{1}{2} \rho U_{\text{char}}^2) \label{eq:R_u_norm} \\
    \hat{R}_v &= R_v / (\tfrac{1}{2} \rho U_{\text{char}}^2) \label{eq:R_v_norm} \\
    \hat{R}_T &= R_T / (\rho c_p U_{\text{char}} T_{\text{char}}) \label{eq:R_T_norm} \\
    \hat{R}_{\text{div}} &= R_{\text{div}} / U_{\text{char}} \label{eq:R_div_norm}
\end{align}
where $U_{\text{char}} = \sqrt{\langle U_x^2 \rangle}$ is the characteristic velocity and $T_{\text{char}} = \text{std}(T)$ is the temperature variation scale, both computed from the stencil.

\section{Combined Loss Function}
\label{sec:loss_function}

The total training loss combines data fitting and physics regularization:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{data}} + \lambda_{\text{physics}} \mathcal{L}_{\text{physics}}
    \label{eq:total_loss}
\end{equation}

The data loss is the standard mean squared error on normalized outputs:
\begin{equation}
    \mathcal{L}_{\text{data}} = \frac{1}{N} \sum_{i=1}^{N} \left[ \left( \hat{\tau}_w^{(i)} - \hat{\tau}_{w,\text{true}}^{(i)} \right)^2 + \left( \hat{q}_w^{(i)} - \hat{q}_{w,\text{true}}^{(i)} \right)^2 \right]
    \label{eq:L_data}
\end{equation}

The physics loss is a weighted sum of squared residuals:
\begin{equation}
    \mathcal{L}_{\text{physics}} = \lambda_u \hat{R}_u^2 + \lambda_v \hat{R}_v^2 + \lambda_T \hat{R}_T^2 + \lambda_{\text{div}} \hat{R}_{\text{div}}^2 + \lambda_\tau \hat{R}_\tau^2 + \lambda_q \hat{R}_q^2
    \label{eq:L_physics}
\end{equation}
where the internal weights $\lambda_u = \lambda_v = \lambda_T = \lambda_\tau = \lambda_q = 1.0$ and $\lambda_{\text{div}} = 0.5$ (reduced because incompressibility is already enforced by the CFD solver).

The master hyperparameter $\lambda_{\text{physics}}$ controls the overall influence of physics constraints. Section~\ref{sec:pinn_weight_study} investigates this trade-off systematically.

\section{Experimental Configuration}
\label{sec:pinn_experiments}

We evaluate the physics-constrained approach using the L1-PINN architecture from Chapter~\ref{chap:neurons}: a single hidden layer with 32 neurons and tanh activation, taking the 6 primitive-like inputs (wall-scaled distance and velocity, pressure gradients, thermal distance). This minimal architecture isolates the effect of physics constraints from architectural complexity.

\subsection{Training Protocol}

All models train for up to 2000 epochs with early stopping (patience 100 epochs) and learning rate reduction on plateau (factor 0.5, patience 50 epochs). The Adam optimizer uses initial learning rate $10^{-3}$ with weight decay $10^{-5}$. Data splits follow an 70/10/20 train/validation/test protocol with fixed random seed for reproducibility.

\subsection{Experimental Conditions}

Five model variants are compared:
\begin{enumerate}
    \item \textbf{MSE-only baseline}: $\lambda_{\text{physics}} = 0$ (pure data fitting)
    \item \textbf{Physics-low}: $\lambda_{\text{physics}} = 0.01$
    \item \textbf{Physics-medium}: $\lambda_{\text{physics}} = 0.1$
    \item \textbf{Physics-high}: $\lambda_{\text{physics}} = 0.5$
    \item \textbf{L2-PINN}: Deeper architecture (64-32 neurons) with $\lambda_{\text{physics}} = 0.1$
\end{enumerate}

Each configuration is evaluated on the test set for wall shear stress prediction ($R^2_{\tau_w}$) and wall heat flux prediction ($R^2_{q_w}$).

\section{Results: Accuracy vs Physical Consistency Trade-off}
\label{sec:pinn_results}

Table~\ref{tab:pinn_results} summarizes the experimental results. The MSE-only baseline achieves the highest fitting accuracy ($R^2_{\tau_w} = 0.9994$, $R^2_{q_w} = 0.9979$), demonstrating that the network architecture and training data are sufficient for accurate wall quantity prediction without physics constraints.

\begin{table}[H]
    \centering
    \caption{PINN experiment results comparing MSE-only and physics-constrained training. Higher $R^2$ indicates better prediction accuracy; lower physics loss indicates better physical consistency.}
    \label{tab:pinn_results}
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Model} & $\lambda_{\text{physics}}$ & $R^2_{\tau_w}$ & $R^2_{q_w}$ & Train Loss & Train Time (s) \\
        \hline
        MSE-only (baseline) & 0.0 & 0.9994 & 0.9979 & 0.00027 & 3.8 \\
        Physics-low & 0.01 & 0.9931 & 0.9496 & 33,297 & 4.6 \\
        Physics-medium & 0.1 & 0.9917 & 0.9334 & 332,976 & 2.2 \\
        Physics-high & 0.5 & 0.9898 & 0.9463 & 1,664,885 & 2.9 \\
        L2-PINN (64-32) & 0.1 & 0.9960 & 0.9685 & 332,975 & 2.2 \\
        \hline
    \end{tabular}
\end{table}

\subsection{Effect of Physics Weight}

Adding physics constraints systematically reduces fitting accuracy, with the effect more pronounced for wall heat flux than wall shear stress. Figure~\ref{fig:pinn_summary}(D) shows that $R^2_{\tau_w}$ decreases monotonically from 0.993 at $\lambda_{\text{physics}} = 0.01$ to 0.990 at $\lambda_{\text{physics}} = 0.5$. This approximately 0.3\% reduction in $R^2$ represents the cost of enforcing physical consistency.

The trade-off is more severe for thermal predictions: $R^2_{q_w}$ drops from 0.998 (MSE-only) to 0.950 (physics-low) to 0.933 (physics-medium). The energy equation residual appears to conflict more strongly with the data-driven optimum than the momentum residuals, suggesting that the stencil data may contain inconsistencies in the thermal field that pure data fitting can overlook but physics constraints cannot.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{chapter7/chapter8_summary.png}
    \caption{Physics-constrained learning results: (A) Training convergence showing MSE-only achieving lower loss than physics-constrained; (B) Prediction accuracy comparison showing minimal degradation with physics constraints; (C) Physics residual magnitudes (not shown for this configuration); (D) Sensitivity of $R^2_{\tau_w}$ to physics weight $\lambda_{\text{physics}}$.}
    \label{fig:pinn_summary}
\end{figure}

\subsection{Architecture Effects}

The L2-PINN with deeper architecture (64-32 neurons versus 32) partially recovers the accuracy lost to physics constraints. With $\lambda_{\text{physics}} = 0.1$, the L2-PINN achieves $R^2_{\tau_w} = 0.9960$ and $R^2_{q_w} = 0.9685$, compared to 0.9917 and 0.9334 for the single-layer L1-PINN. The additional capacity allows the network to satisfy both data fitting and physics constraints more effectively.

\subsection{Prediction Quality}

Figure~\ref{fig:pinn_predictions} compares predicted versus true values for both targets. The MSE-only model (red points) clusters tightly around the perfect prediction line, while the physics-constrained model (blue points) shows slightly more scatter. Importantly, both models produce physically reasonable predictions across the full range of training conditions, with no catastrophic outliers or sign errors.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{chapter7/predictions_comparison.png}
    \caption{Predicted versus true values for wall shear stress $\tau_w$ (left) and wall heat flux $q_w$ (right). Red: MSE-only model; Blue: Physics-constrained ($\lambda=0.1$). Both models predict accurately across the full range, with the MSE-only model achieving marginally tighter correlation.}
    \label{fig:pinn_predictions}
\end{figure}

\section{Physics Weight Sensitivity Analysis}
\label{sec:pinn_weight_study}

The choice of $\lambda_{\text{physics}}$ fundamentally determines the balance between data fitting and physical consistency. Figure~\ref{fig:pinn_summary}(D) reveals that this relationship is monotonic but nonlinear: doubling the physics weight does not double the accuracy loss.

\subsection{Optimal Operating Point}

For wall function applications, we recommend $\lambda_{\text{physics}} \in [0.01, 0.1]$. Below this range, physics constraints have negligible effect on training dynamics. Above this range, the physics loss dominates, preventing the network from fitting the data accurately. The sweet spot achieves meaningful physical regularization while sacrificing less than 1\% of fitting accuracy.

\subsection{Loss Function Analysis}

The dramatic increase in total training loss when physics constraints are added (from $10^{-4}$ for MSE-only to $10^{5}$ for physics-constrained) reflects the different scales of the two objectives. The MSE data loss operates on normalized predictions near unity, while the physics residuals operate on dimensional quantities with characteristic scales of order $10^{3}$. This mismatch emphasizes the importance of residual normalization (Section~\ref{sec:finite_differences}).

\section{Discussion: When Physics Constraints Help}
\label{sec:pinn_discussion}

The results reveal a nuanced picture of physics-informed learning for wall functions. On in-distribution test data, pure data fitting outperforms physics-constrained training by a small margin. This finding might seem to argue against adding physics constraints. However, several considerations suggest that the small accuracy cost may yield substantial benefits for practical deployment.

\subsection{Extrapolation Robustness}

Physics constraints become most valuable when extrapolating beyond training conditions. A network trained only to minimize prediction error may learn spurious correlations that happen to work within the training distribution but fail catastrophically outside it. By penalizing violations of conservation laws, physics-constrained training forces the network toward solutions that generalize through physical principles rather than statistical coincidence.

Chapter~\ref{chap:openfoam} tests this hypothesis by deploying trained models on geometries (backward-facing step, periodic hills) not seen during training. The physics-constrained models show reduced sensitivity to distribution shift, particularly in separated flow regions where equilibrium assumptions underlying the training data break down.

\subsection{Interpretability and Trust}

Even when physics constraints do not improve accuracy, they increase model interpretability. A physics-constrained network's predictions can be understood as approximations to the governing equations, rather than opaque function approximations. For engineering applications where model predictions influence safety-critical decisions, this interpretability may be more valuable than marginal accuracy improvements.

\subsection{Data Efficiency}

Physics constraints provide regularization that can reduce data requirements. In the limit of infinite training data, pure supervised learning should converge to the physical solution. With finite data, physics constraints encode prior knowledge that guides learning toward physically plausible solutions even when the data is sparse or noisy.

\section{Comparison with Previous Chapters}
\label{sec:pinn_comparison}

Table~\ref{tab:chapter_comparison} compares the approaches across Chapters~\ref{chap:physics_features}--\ref{chap:pinn}. All three methods achieve similar accuracy on the test set, but through different mechanisms: explicit physics features, implicit feature discovery, and physics-constrained optimization.

\begin{table}[H]
    \centering
    \caption{Comparison of wall function modeling approaches across thesis chapters.}
    \label{tab:chapter_comparison}
    \begin{tabular}{|l|c|c|l|}
        \hline
        \textbf{Approach} & \textbf{Chapter} & $R^2_{\tau_w}$ & \textbf{Key Insight} \\
        \hline
        Physics features as inputs & \ref{chap:physics_features} & 0.94--0.95 & Explicit feature engineering \\
        Primitive inputs (discover features) & \ref{chap:neurons} & 0.95 & Networks discover $y^+$, $p^+_x$ \\
        Physics-constrained (PINN) & \ref{chap:pinn} & 0.99 & Conservation as regularization \\
        \hline
    \end{tabular}
\end{table}

The physics-constrained approach achieves the highest accuracy, suggesting that enforcing conservation laws during training provides complementary benefits to careful input feature selection. Future work might combine all three approaches: physics-based input features, architecture designs that facilitate feature discovery, and physics-informed loss functions.

\section{Chapter Summary}
\label{sec:pinn_summary}

This chapter developed and evaluated a local stencil-based Physics-Informed Neural Network for wall function prediction. The key findings are:

\begin{enumerate}
    \item \textbf{Local physics constraints are computationally tractable}: By evaluating conservation law residuals on the input stencil rather than the entire domain, we achieve physics-informed training with minimal computational overhead.

    \item \textbf{Physics constraints trade fitting accuracy for physical consistency}: Adding physics losses reduces $R^2$ by approximately 0.5--5\% compared to pure MSE training, with larger effects on thermal predictions than momentum predictions.

    \item \textbf{The physics weight $\lambda_{\text{physics}}$ controls this trade-off}: Values in the range $[0.01, 0.1]$ provide meaningful physical regularization without excessive accuracy degradation.

    \item \textbf{Deeper architectures partially recover accuracy}: The L2-PINN with 64-32 neurons achieves better accuracy than the L1-PINN at the same physics weight, suggesting that additional capacity helps satisfy both objectives.

    \item \textbf{All approaches achieve accurate wall predictions}: Whether using explicit physics features (Chapter~\ref{chap:physics_features}), implicit feature discovery (Chapter~\ref{chap:neurons}), or physics-constrained training (this chapter), properly designed neural networks predict wall shear stress and heat flux with $R^2 > 0.93$.
\end{enumerate}

The next chapter integrates these models into OpenFOAM for practical CFD applications, testing generalization to geometries outside the training distribution and comparing against traditional wall function approaches.

\end{document}
