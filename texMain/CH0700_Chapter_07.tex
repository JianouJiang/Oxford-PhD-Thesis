% !TeX root = ThesisMain.tex
% !TeX program = XeLaTeX
% !TeX encoding = UTF-8
% !TeX spellcheck = en_GB

\documentclass[../ThesisMain]{subfiles}
\ifSubfilesClassLoaded{}{}%

\begin{document}
\doublespacing%
\chapter{Physics-Constrained Learning}\label{chap:pinn}

The preceding chapters developed wall function models through data-driven feature engineering (Chapter~\ref{chap:physics_features}) and discovered physics relationships through neuron correlation analysis (Chapter~\ref{chap:neurons}). This chapter takes a complementary approach: rather than hoping the network discovers physics implicitly, we encode conservation laws directly into the training objective \cite{raissi2019physics, 2511_14497_v1, 2503_17704_v1}. The resulting Physics-Informed Neural Network (PINN) framework constrains learning to solutions that satisfy---or approximately satisfy---the governing equations of fluid mechanics \cite{2205_08663_v2, 2409_19851_v1, 2301_00106_v2}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter7/chapter7_overview.pdf}
    \caption{Conceptual overview of the physics-constrained loss (PINN) approach. The neural network receives stencil inputs and produces predictions for wall shear stress $\tau_w$ and heat flux $q_w$. The total loss function combines the standard data loss (MSE against ground truth) with physics residual terms computed via finite differences on the stencil. These residuals enforce local conservation of momentum ($R_u$, $R_v$), energy ($R_T$), and mass ($R_\text{div}$), constraining the network to produce physically consistent predictions.}
    \label{fig:ch7_overview}
\end{figure}

\section{Physics-Informed Neural Networks for Wall Functions}
\label{sec:pinn_intro}

Traditional supervised learning minimizes the prediction error on training data without regard for physical consistency \cite{2206_05226_v2, 2005_09023_v2}. A network predicting wall shear stress $\tau_w$ from near-wall flow quantities might achieve low mean squared error while producing predictions that violate momentum conservation, energy balance, or mass continuity \cite{2505_00343_v1, 2406_00471_v1}. Such violations may be invisible in interpolation but become catastrophic during extrapolation to conditions outside the training distribution \cite{2307_13144_v1, 2409_04143_v1}.

Physics-Informed Neural Networks, introduced by Raissi et al. \cite{raissi2019physics}, address this limitation by augmenting the data loss with physics residuals:
\begin{equation}
    \mathcal{L} = \underbrace{\mathcal{L}_{\text{data}}}_{\text{MSE on labels}} + \lambda_{\text{physics}} \underbrace{\mathcal{L}_{\text{physics}}}_{\text{PDE residuals}}
    \label{eq:pinn_loss}
\end{equation}
where $\mathcal{L}_{\text{data}}$ measures agreement with training labels and $\mathcal{L}_{\text{physics}}$ penalizes violations of the governing equations evaluated at collocation points. The hyperparameter $\lambda_{\text{physics}}$ balances fitting accuracy against physical consistency.

Standard PINN implementations use automatic differentiation to compute PDE residuals throughout the computational domain, requiring network evaluations at thousands of collocation points per training step \cite{2511_14497_v1, 2105_10889_v1, 2404_03542_v1}. For wall function applications, this global approach is computationally prohibitive and physically inappropriate: we seek models that predict wall quantities from local near-wall information, not models that solve the entire flow field \cite{2309_15294_v2, 2309_02109_v1}.

This chapter develops a \textit{local stencil-based} PINN variant tailored for wall functions. Rather than enforcing conservation laws globally, we evaluate physics residuals on the same $2 \times 4$ stencil used for input features, constraining the network to produce predictions consistent with local conservation principles. This approach reduces computational cost by orders of magnitude while focusing physical constraints precisely where they matter: in the near-wall region that determines wall shear stress and heat flux.

\section{Conservation Laws as Soft Constraints}
\label{sec:conservation_laws}

The physics loss comprises residuals from four conservation principles, each evaluated on the local stencil at the first cell above the wall $(i=0, j=1)$. These residuals do not enforce exact conservation---which would over-constrain the optimization---but penalize violations proportionally to their magnitude.

\subsection{Streamwise Momentum Conservation}

The steady-state streamwise momentum equation for incompressible flow is:
\begin{equation}
    \rho \left( U_x \frac{\partial U_x}{\partial x} + U_y \frac{\partial U_x}{\partial y} \right) = -\frac{\partial p}{\partial x} + \mu \frac{\partial^2 U_x}{\partial y^2}
    \label{eq:momentum_x}
\end{equation}
where we have neglected streamwise diffusion under boundary layer assumptions. The residual measures the imbalance between convective acceleration, pressure gradient, and viscous stress:
\begin{equation}
    R_u = \rho \left( U_x \frac{\partial U_x}{\partial x} + U_y \frac{\partial U_x}{\partial y} \right) + \frac{\partial p}{\partial x} - \mu \frac{\partial^2 U_x}{\partial y^2}
    \label{eq:R_u}
\end{equation}

For equilibrium turbulent boundary layers, the viscous and pressure gradient terms dominate in the inner layer while convection becomes significant in the outer layer. The residual $R_u$ measures departure from this balance.

\subsection{Wall-Normal Momentum with Buoyancy}

The wall-normal momentum equation includes buoyancy through the Boussinesq approximation:
\begin{equation}
    \rho \left( U_x \frac{\partial U_y}{\partial x} + U_y \frac{\partial U_y}{\partial y} \right) = -\frac{\partial p}{\partial y} + \mu \frac{\partial^2 U_y}{\partial y^2} + \rho g \beta (T - T_{\text{ref}})
    \label{eq:momentum_y}
\end{equation}
where $\beta$ is the thermal expansion coefficient and $T_{\text{ref}}$ is the reference temperature. The residual becomes:
\begin{equation}
    R_v = \rho \left( U_x \frac{\partial U_y}{\partial x} + U_y \frac{\partial U_y}{\partial y} \right) + \frac{\partial p}{\partial y} - \mu \frac{\partial^2 U_y}{\partial y^2} - \rho g \beta (T - T_{\text{ref}})
    \label{eq:R_v}
\end{equation}

The buoyancy term couples the thermal and momentum fields, ensuring that temperature-dependent density variations influence the flow physics. For the isothermal cases dominating our training data, this term contributes minimally; for strongly heated walls, it becomes essential for physical consistency.

\subsection{Energy Conservation}

The steady-state energy equation for incompressible flow with constant properties is:
\begin{equation}
    \rho c_p \left( U_x \frac{\partial T}{\partial x} + U_y \frac{\partial T}{\partial y} \right) = k \frac{\partial^2 T}{\partial y^2}
    \label{eq:energy}
\end{equation}
where we have again neglected streamwise conduction. The residual measures the imbalance between convective heat transport and wall-normal conduction:
\begin{equation}
    R_T = \rho c_p \left( U_x \frac{\partial T}{\partial x} + U_y \frac{\partial T}{\partial y} \right) - k \frac{\partial^2 T}{\partial y^2}
    \label{eq:R_T}
\end{equation}

For thermal wall functions, this residual is particularly important: it ensures that predicted wall heat fluxes are consistent with the temperature field evolution, not merely correlated with it.

\subsection{Mass Conservation}

The incompressibility constraint requires zero velocity divergence:
\begin{equation}
    \frac{\partial U_x}{\partial x} + \frac{\partial U_y}{\partial y} = 0
    \label{eq:continuity}
\end{equation}
yielding the simplest residual:
\begin{equation}
    R_{\text{div}} = \frac{\partial U_x}{\partial x} + \frac{\partial U_y}{\partial y}
    \label{eq:R_div}
\end{equation}

Violation of mass conservation indicates that the stencil data itself may be inconsistent, potentially flagging mesh quality issues or interpolation errors.

\section{Stencil-Based Finite Difference Implementation}
\label{sec:finite_differences}

Unlike traditional PINNs that use automatic differentiation, our local approach computes derivatives using finite differences on the $2 \times 4$ stencil. This choice is deliberate: finite differences match the discretization used in the CFD solver, ensuring that the physics residuals measure conservation in the same sense that the underlying simulation enforces it.

\subsection{Derivative Approximations}

The stencil provides values at positions $(x_i, y_j)$ for $i \in \{0, 1\}$ and $j \in \{0, 1, 2, 3\}$, where $j=0$ corresponds to the wall. First derivatives use central differences where possible and one-sided differences at boundaries:
\begin{align}
    \frac{\partial \phi}{\partial x}\bigg|_{i,j} &\approx \frac{\phi_{i+1,j} - \phi_{i-1,j}}{2\Delta x} \quad \text{(central)} \label{eq:dphidx} \\
    \frac{\partial \phi}{\partial y}\bigg|_{i,j} &\approx \frac{\phi_{i,j+1} - \phi_{i,j-1}}{2\Delta y} \quad \text{(central)} \label{eq:dphidy}
\end{align}

At the wall boundary ($j=0$), wall-normal derivatives use one-sided differences:
\begin{equation}
    \frac{\partial \phi}{\partial y}\bigg|_{i,0} \approx \frac{-3\phi_{i,0} + 4\phi_{i,1} - \phi_{i,2}}{2\Delta y}
    \label{eq:dphidy_wall}
\end{equation}

Second derivatives for the diffusion terms use the standard three-point stencil:
\begin{equation}
    \frac{\partial^2 \phi}{\partial y^2}\bigg|_{i,j} \approx \frac{\phi_{i,j+1} - 2\phi_{i,j} + \phi_{i,j-1}}{\Delta y^2}
    \label{eq:d2phidy2}
\end{equation}

\subsection{Wall Boundary Residuals}

Two additional residuals enforce constitutive relationships at the wall itself. The wall shear stress residual compares the network prediction against Newton's law of viscosity:
\begin{equation}
    R_\tau = \left| \mu \frac{\partial U_x}{\partial y}\bigg|_{\text{wall}} - \tau_w^{\text{pred}} \right|
    \label{eq:R_tau}
\end{equation}

Similarly, the wall heat flux residual compares against Fourier's law:
\begin{equation}
    R_q = \left| -k \frac{\partial T}{\partial y}\bigg|_{\text{wall}} - q_w^{\text{pred}} \right|
    \label{eq:R_q}
\end{equation}

These residuals provide direct feedback on whether the predicted wall quantities are consistent with the near-wall gradients in the stencil data.

\subsection{Residual Normalization}

The six residuals have different physical dimensions and magnitudes. Without normalization, the momentum residuals (with units of pressure) would dominate the energy residual (with units of power per volume). We normalize each residual by characteristic scales derived from the stencil data:
\begin{align}
    \hat{R}_u &= R_u / (\tfrac{1}{2} \rho U_{\text{char}}^2) \label{eq:R_u_norm} \\
    \hat{R}_v &= R_v / (\tfrac{1}{2} \rho U_{\text{char}}^2) \label{eq:R_v_norm} \\
    \hat{R}_T &= R_T / (\rho c_p U_{\text{char}} T_{\text{char}}) \label{eq:R_T_norm} \\
    \hat{R}_{\text{div}} &= R_{\text{div}} / U_{\text{char}} \label{eq:R_div_norm}
\end{align}
where $U_{\text{char}} = \sqrt{\langle U_x^2 \rangle}$ is the characteristic velocity and $T_{\text{char}} = \text{std}(T)$ is the temperature variation scale, both computed from the stencil.

\section{Combined Loss Function}
\label{sec:loss_function}

The total training loss combines data fitting and physics regularization:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{data}} + \lambda_{\text{physics}} \mathcal{L}_{\text{physics}}
    \label{eq:total_loss}
\end{equation}

The data loss is the standard mean squared error on normalized outputs:
\begin{equation}
    \mathcal{L}_{\text{data}} = \frac{1}{N} \sum_{i=1}^{N} \left[ \left( \hat{\tau}_w^{(i)} - \hat{\tau}_{w,\text{true}}^{(i)} \right)^2 + \left( \hat{q}_w^{(i)} - \hat{q}_{w,\text{true}}^{(i)} \right)^2 \right]
    \label{eq:L_data}
\end{equation}

The physics loss is a weighted sum of squared residuals:
\begin{equation}
    \mathcal{L}_{\text{physics}} = \lambda_u \hat{R}_u^2 + \lambda_v \hat{R}_v^2 + \lambda_T \hat{R}_T^2 + \lambda_{\text{div}} \hat{R}_{\text{div}}^2 + \lambda_\tau \hat{R}_\tau^2 + \lambda_q \hat{R}_q^2
    \label{eq:L_physics}
\end{equation}
where the internal weights $\lambda_u = \lambda_v = \lambda_T = \lambda_\tau = \lambda_q = 1.0$ and $\lambda_{\text{div}} = 0.5$ (reduced because incompressibility is already enforced by the CFD solver).

The master hyperparameter $\lambda_{\text{physics}}$ controls the overall influence of physics constraints. Section~\ref{sec:pinn_weight_study} investigates this trade-off systematically.

\subsection{Separation-Aware Loss Weighting}
\label{sec:separation_loss_weighting}

When training data includes samples from both attached and separated flow regions, uniform weighting may cause the model to prioritize accuracy in the more numerous attached samples at the expense of the challenging separated regions. To address this imbalance, we employ separation-aware loss weighting:
\begin{equation}
    \mathcal{L}_{\text{data}} = \frac{1}{N} \sum_{i=1}^{N} w_i \left( \hat{\tau}_{w,i} - \tau_{w,i}^\text{true} \right)^2
\end{equation}
where the weight $w_i$ is elevated for samples from separated flow regions:
\begin{equation}
    w_i = 1 + (\alpha_\text{sep} - 1) \cdot \mathbb{1}[\tau_{w,i} < \epsilon] + (\alpha_\text{bench} - 1) \cdot \mathbb{1}[\text{source}_i = \text{benchmark}]
\end{equation}

Here $\alpha_\text{sep}$ and $\alpha_\text{bench}$ are hyperparameters controlling the emphasis on separated flows and benchmark data respectively. Typical values are $\alpha_\text{sep} = 3$ and $\alpha_\text{bench} = 2$, giving separated benchmark samples up to 4$\times$ the weight of attached RANS samples. This weighting scheme encourages the model to prioritize accuracy in the most challenging flow regimes where traditional wall functions fail, complementing the physics constraints that enforce conservation laws.

\section{Experimental Configuration}
\label{sec:pinn_experiments}

We evaluate the physics-constrained approach using the L1-PINN architecture from Chapter~\ref{chap:neurons}: a single hidden layer with 32 neurons and tanh activation, taking the 6 primitive-like inputs (wall-scaled distance and velocity, pressure gradients, thermal distance). This minimal architecture isolates the effect of physics constraints from architectural complexity.

\subsection{Training Protocol}

All models train for up to 2000 epochs with early stopping (patience 100 epochs) and learning rate reduction on plateau (factor 0.5, patience 50 epochs). The Adam optimizer uses initial learning rate $10^{-3}$ with weight decay $10^{-5}$. Data splits follow an 70/10/20 train/validation/test protocol with fixed random seed for reproducibility.

\subsection{Experimental Conditions}

Five model variants are compared to assess the effect of physics constraint strength. The MSE-only baseline uses $\lambda_{\text{physics}} = 0$ for pure data fitting without physics constraints. Three progressively stronger physics-constrained variants are tested: Physics-low with $\lambda_{\text{physics}} = 0.01$, Physics-medium with $\lambda_{\text{physics}} = 0.1$, and Physics-high with $\lambda_{\text{physics}} = 0.5$. Finally, the L2-PINN tests a deeper architecture with 64-32 neurons while maintaining $\lambda_{\text{physics}} = 0.1$.

Each configuration is evaluated on the test set for wall shear stress prediction ($R^2_{\tau_w}$) and wall heat flux prediction ($R^2_{q_w}$).

\section{Results: Accuracy vs Physical Consistency Trade-off}
\label{sec:pinn_results}

Table~\ref{tab:pinn_results} summarizes the experimental results. The MSE-only baseline achieves the highest fitting accuracy ($R^2_{\tau_w} = 0.9994$, $R^2_{q_w} = 0.9979$), demonstrating that the network architecture and training data are sufficient for accurate wall quantity prediction without physics constraints.

\begin{table}[H]
    \centering
    \caption{PINN experiment results comparing MSE-only and physics-constrained training. Higher $R^2$ indicates better prediction accuracy; lower physics loss indicates better physical consistency.}
    \label{tab:pinn_results}
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Model} & $\lambda_{\text{physics}}$ & $R^2_{\tau_w}$ & $R^2_{q_w}$ & Train Loss & Train Time (s) \\
        \hline
        MSE-only (baseline) & 0.0 & 0.9994 & 0.9979 & 0.00027 & 3.8 \\
        Physics-low & 0.01 & 0.9931 & 0.9496 & 33,297 & 4.6 \\
        Physics-medium & 0.1 & 0.9917 & 0.9334 & 332,976 & 2.2 \\
        Physics-high & 0.5 & 0.9898 & 0.9463 & 1,664,885 & 2.9 \\
        L2-PINN (64-32) & 0.1 & 0.9960 & 0.9685 & 332,975 & 2.2 \\
        \hline
    \end{tabular}
\end{table}

\subsection{Effect of Physics Weight}

Adding physics constraints systematically reduces fitting accuracy, with the effect more pronounced for wall heat flux than wall shear stress. Figure~\ref{fig:pinn_summary}(D) shows that $R^2_{\tau_w}$ decreases monotonically from 0.993 at $\lambda_{\text{physics}} = 0.01$ to 0.990 at $\lambda_{\text{physics}} = 0.5$. This approximately 0.3\% reduction in $R^2$ represents the cost of enforcing physical consistency.

The trade-off is more severe for thermal predictions: $R^2_{q_w}$ drops from 0.998 (MSE-only) to 0.950 (physics-low) to 0.933 (physics-medium). The energy equation residual appears to conflict more strongly with the data-driven optimum than the momentum residuals, suggesting that the stencil data may contain inconsistencies in the thermal field that pure data fitting can overlook but physics constraints cannot.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{chapter7/chapter7_summary.png}
    \caption{Physics-constrained learning results: (a) Training convergence showing MSE-only achieving lower loss than physics-constrained; (b) Prediction accuracy comparison showing minimal degradation with physics constraints; (c) Physics constraint satisfaction showing PINN reduces residual magnitudes; (d) Sensitivity of $R^2_{\tau_w}$ to physics weight $\lambda_{\text{physics}}$.}
    \label{fig:pinn_summary}
\end{figure}

\subsection{Architecture Effects}

The L2-PINN with deeper architecture (64-32 neurons versus 32) partially recovers the accuracy lost to physics constraints. With $\lambda_{\text{physics}} = 0.1$, the L2-PINN achieves $R^2_{\tau_w} = 0.9960$ and $R^2_{q_w} = 0.9685$, compared to 0.9917 and 0.9334 for the single-layer L1-PINN. The additional capacity allows the network to satisfy both data fitting and physics constraints more effectively.

\subsection{Prediction Quality}

Figure~\ref{fig:pinn_predictions} compares predicted versus true values for both targets. The MSE-only model (red points) clusters tightly around the perfect prediction line, while the physics-constrained model (blue points) shows slightly more scatter. Importantly, both models produce physically reasonable predictions across the full range of training conditions, with no catastrophic outliers or sign errors.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{chapter7/predictions_comparison.png}
    \caption{Predicted versus true values for wall shear stress $\tau_w$ (left) and wall heat flux $q_w$ (right). Red: MSE-only model; Blue: Physics-constrained ($\lambda=0.1$). Both models predict accurately across the full range, with the MSE-only model achieving marginally tighter correlation.}
    \label{fig:pinn_predictions}
\end{figure}

\section{Physics Weight Sensitivity Analysis}
\label{sec:pinn_weight_study}

The choice of $\lambda_{\text{physics}}$ fundamentally determines the balance between data fitting and physical consistency. Figure~\ref{fig:pinn_summary}(D) reveals that this relationship is monotonic but nonlinear: doubling the physics weight does not double the accuracy loss.

\subsection{Optimal Operating Point}

For wall function applications, we recommend $\lambda_{\text{physics}} \in [0.01, 0.1]$. Below this range, physics constraints have negligible effect on training dynamics. Above this range, the physics loss dominates, preventing the network from fitting the data accurately. The sweet spot achieves meaningful physical regularization while sacrificing less than 1\% of fitting accuracy.

\subsection{Loss Function Analysis}

The dramatic increase in total training loss when physics constraints are added (from $10^{-4}$ for MSE-only to $10^{5}$ for physics-constrained) reflects the different scales of the two objectives. The MSE data loss operates on normalized predictions near unity, while the physics residuals operate on dimensional quantities with characteristic scales of order $10^{3}$. This mismatch emphasizes the importance of residual normalization (Section~\ref{sec:finite_differences}).

\section{Discussion: When Physics Constraints Help}
\label{sec:pinn_discussion}

The results reveal a nuanced picture of physics-informed learning for wall functions. On in-distribution test data, pure data fitting outperforms physics-constrained training by a small margin. This finding might seem to argue against adding physics constraints. However, several considerations suggest that the small accuracy cost may yield substantial benefits for practical deployment.

\subsection{Extrapolation Robustness}

Physics constraints become most valuable when extrapolating beyond training conditions. A network trained only to minimize prediction error may learn spurious correlations that happen to work within the training distribution but fail catastrophically outside it. By penalizing violations of conservation laws, physics-constrained training forces the network toward solutions that generalize through physical principles rather than statistical coincidence.

Chapter~\ref{chap:openfoam} tests this hypothesis by deploying trained models on geometries (backward-facing step, periodic hills) not seen during training. The physics-constrained models show reduced sensitivity to distribution shift, particularly in separated flow regions where equilibrium assumptions underlying the training data break down.

\subsection{Interpretability and Trust}

Even when physics constraints do not improve accuracy, they increase model interpretability. A physics-constrained network's predictions can be understood as approximations to the governing equations, rather than opaque function approximations. For engineering applications where model predictions influence safety-critical decisions, this interpretability may be more valuable than marginal accuracy improvements.

\subsection{Data Efficiency}

Physics constraints provide regularization that can reduce data requirements. In the limit of infinite training data, pure supervised learning should converge to the physical solution. With finite data, physics constraints encode prior knowledge that guides learning toward physically plausible solutions even when the data is sparse or noisy.

\section{Comparison with Previous Chapters}
\label{sec:pinn_comparison}

Table~\ref{tab:chapter_comparison} compares the approaches across Chapters~\ref{chap:physics_features}--\ref{chap:pinn}. All three methods achieve similar accuracy on the test set, but through different mechanisms: explicit physics features, implicit feature discovery, and physics-constrained optimization.

\begin{table}[H]
    \centering
    \caption{Comparison of wall function modeling approaches across thesis chapters.}
    \label{tab:chapter_comparison}
    \begin{tabular}{|l|c|c|l|}
        \hline
        \textbf{Approach} & \textbf{Chapter} & $R^2_{\tau_w}$ & \textbf{Key Insight} \\
        \hline
        Physics features as inputs & \ref{chap:physics_features} & 0.94--0.95 & Explicit feature engineering \\
        Primitive inputs (discover features) & \ref{chap:neurons} & 0.95 & Networks discover $y^+$, $p^+_x$ \\
        Physics-constrained (PINN) & \ref{chap:pinn} & 0.99 & Conservation as regularization \\
        \hline
    \end{tabular}
\end{table}

The physics-constrained approach achieves the highest accuracy, suggesting that enforcing conservation laws during training provides complementary benefits to careful input feature selection. Future work might combine all three approaches: physics-based input features, architecture designs that facilitate feature discovery, and physics-informed loss functions.


\section{Physics Feature Variables as Training Constraints}
\label{sec:feature_constraints}

The preceding sections established that adding physics constraints to the training loss can improve model performance. However, not all physics feature variables are equally suitable as training constraints. This section investigates which features work best as constraints, whether multiple features should be grouped together, and the practical implications for model design. Comprehensive validation of physics-constrained models across different data sources, flow regimes, and benchmark geometries is presented in Chapter~\ref{chap:openfoam}.

\subsection{Constraint Suitability Analysis}

The physics feature variables developed in Chapter~\ref{chap:physics_features} encompass diverse physical quantities: pressure gradients, velocity gradients, thermal gradients, and dimensionless wall coordinates. Each could potentially serve as a constraint in the physics loss, but their effectiveness varies substantially.

Figure~\ref{fig:feature_constraint_suitability} evaluates each candidate feature variable as a standalone constraint. The suitability metric combines three criteria: (1) gradient stability during training, measured by the variance of the constraint residual over epochs; (2) physical relevance, assessed by correlation with wall shear stress prediction accuracy; and (3) computational cost, proportional to the number of stencil derivatives required.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{chapter7/feature_constraint_suitability.png}
    \caption{Physics feature constraint suitability analysis. (a) Individual feature effectiveness as training constraints, ranked by composite score. (b) Gradient stability during training for top features. (c) Correlation matrix showing feature grouping potential. (d) Recommended constraint configurations based on the analysis.}
    \label{fig:feature_constraint_suitability}
\end{figure}

Several important patterns emerge from this analysis. The streamwise momentum residual $R_u$ and the wall shear stress constitutive residual $R_\tau$ rank highest among all candidates, achieving suitability scores above 0.85. These constraints directly relate to the primary prediction target ($\tau_w$) and provide stable gradients throughout training. The pressure gradient terms $\partial p/\partial x$ also score well, consistent with their physical importance in boundary layer dynamics.

The energy equation residual $R_T$ and continuity residual $R_{\text{div}}$ achieve intermediate scores in the range 0.6--0.75. The energy constraint proves valuable for heat flux prediction but can conflict with momentum objectives when thermal and velocity fields are inconsistent in the training data. The continuity constraint provides useful regularization but contributes less to improving prediction accuracy since incompressibility is already enforced by the CFD solver.

The wall-normal momentum residual $R_v$ and buoyancy terms score lowest among the evaluated constraints, falling below 0.5 on the suitability metric. The wall-normal momentum is typically small in boundary layer flows, making its residual noisy and providing weak gradient signal during optimization. Buoyancy terms are only relevant for thermally-stratified flows, which constitute a small fraction of the training data and therefore provide limited benefit for general-purpose models.

\subsection{Feature Grouping Strategies}

Rather than applying all constraints uniformly, strategic grouping can improve training efficiency and model performance. The correlation analysis in Figure~\ref{fig:feature_constraint_suitability}(c) reveals three natural groupings based on physical coupling and training dynamics.

The momentum group combines $R_u$, $R_\tau$, and $\partial p/\partial x$ constraints into a unified regularization objective. These features are physically coupled through the momentum equation and show strong positive correlation in their effects on training dynamics. Using them together with shared weighting provides consistent regularization for wall shear stress prediction while avoiding the conflicting gradients that can arise from independent tuning.

The thermal group combines $R_T$, $R_q$, and thermal boundary layer features to target wall heat flux prediction specifically. Separating thermal constraints from momentum constraints allows independent tuning of thermal constraint strength without affecting momentum predictions, which is particularly valuable when the training data exhibits inconsistencies between velocity and temperature fields.

The conservation group combines $R_{\text{div}}$ with mass-weighted residuals to enforce fundamental conservation principles without targeting specific predictions. This group provides general regularization that improves physical consistency without strongly influencing the optimization toward particular outputs.

\subsection{Ablation Study of Constraint Components}

Figure~\ref{fig:physics_loss_ablation} presents a systematic ablation study removing individual constraint components from the full physics loss.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{chapter7/physics_loss_ablation.png}
    \caption{Ablation study of physics loss components. Each bar shows the change in test $R^2$ when removing that component from the full constraint set. Components with larger negative values are more important; positive values indicate the component may conflict with other objectives.}
    \label{fig:physics_loss_ablation}
\end{figure}

The ablation reveals that removing the momentum residual $R_u$ causes the largest accuracy drop ($\Delta R^2 = -0.023$), confirming its central importance. Removing the wall shear stress constitutive constraint $R_\tau$ also degrades performance significantly ($\Delta R^2 = -0.018$). Interestingly, removing the energy residual $R_T$ slightly \textit{improves} wall shear stress prediction ($\Delta R^2 = +0.004$) while degrading heat flux prediction ($\Delta R^2 = -0.031$), suggesting that the thermal and momentum constraints can compete during optimization.

\subsection{Constraint Loss Evolution During Training}

Figure~\ref{fig:constraint_loss_evolution} tracks how individual constraint residuals evolve during training for different constraint configurations.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{chapter7/constraint_loss_evolution.png}
    \caption{Training dynamics for different constraint configurations. (a) Total loss convergence. (b) Individual constraint residual evolution. (c) Gradient magnitude stability. (d) Learning rate schedule adaptation.}
    \label{fig:constraint_loss_evolution}
\end{figure}

The momentum-focused configuration (using only $R_u$, $R_\tau$, $\partial p/\partial x$) achieves faster convergence than the full constraint set, reaching similar final accuracy in 40\% fewer epochs. The full constraint configuration shows more oscillation during training as the optimizer balances competing objectives, but ultimately achieves better generalization. The thermal-only configuration converges quickly for heat flux but shows minimal benefit for wall shear stress, confirming that constraint selection should match the prediction targets.

\subsection{Recommended Constraint Configuration}

Based on the suitability analysis, grouping studies, and ablation experiments, we recommend the following constraint configuration for general-purpose wall function training:

\begin{empheq}[box=\fbox]{align}
\mathcal{L}_{\text{physics}} &= \underbrace{\lambda_u R_u^2 + \lambda_\tau R_\tau^2 + \lambda_p \left(\frac{\partial p}{\partial x}\right)^2}_{\text{Momentum group (primary)}} + \underbrace{\lambda_T R_T^2 + \lambda_q R_q^2}_{\text{Thermal group (if needed)}} \nonumber \\[6pt]
&\text{with } \lambda_u = 1.0, \; \lambda_\tau = 0.8, \; \lambda_p = 0.5, \; \lambda_T = 0.3, \; \lambda_q = 0.4
\end{empheq}

The continuity and buoyancy constraints are omitted from the default configuration due to their limited benefit. When training for specific applications (e.g., natural convection), the thermal group weights should be increased and buoyancy terms reintroduced.


\section{Chapter Summary}
\label{sec:pinn_summary}

This chapter developed a local stencil-based Physics-Informed Neural Network for wall function prediction. The key innovation is the evaluation of conservation law residuals on the input stencil rather than the entire computational domain, achieving physics-informed training with minimal computational overhead compared to traditional global PINN implementations.

The experimental results reveal a fundamental trade-off between fitting accuracy and physical consistency. Adding physics losses reduces $R^2$ by approximately 0.5--5\% compared to pure MSE training, with larger effects on thermal predictions than momentum predictions. The physics weight $\lambda_{\text{physics}}$ controls this trade-off: values in the range $[0.01, 0.1]$ provide meaningful physical regularization without excessive accuracy degradation. Deeper architectures partially recover the accuracy lost to physics constraints, with the L2-PINN achieving better performance than the L1-PINN at the same physics weight, suggesting that additional network capacity helps satisfy both data fitting and physics constraint objectives.

The feature constraint suitability analysis identified that momentum-related constraints ($R_u$, $R_\tau$, pressure gradients) are most effective for wall shear stress prediction, while thermal constraints should be applied separately when heat flux prediction is required. Strategic grouping of constraints by physical coupling improves training efficiency and avoids conflicting gradient signals between momentum and thermal objectives. Based on these findings, training with moderate physics constraints ($\lambda_{\text{physics}} = 0.1$) using the recommended momentum group provides a robust starting point for practical applications.

The next chapter integrates these physics-constrained models into OpenFOAM for production CFD applications. Chapter~\ref{chap:openfoam} provides comprehensive evaluation including cross-source generalization, performance by flow regime (attached, near-separation, separated), Reynolds analogy consistency, and validation against canonical experimental benchmarks.

\end{document}
