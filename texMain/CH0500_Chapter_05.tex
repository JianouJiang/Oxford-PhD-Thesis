% !TeX root = ThesisMain.tex
% !TeX program = XeLaTeX
% !TeX encoding = UTF-8
% !TeX spellcheck = en_GB

\documentclass[../ThesisMain]{subfiles}
\ifSubfilesClassLoaded{}{}%

\begin{document}
\doublespacing%
\chapter{Physics-Based Feature Variables as Network Inputs}\label{chap:physics_features}

Chapter~\ref{chap:baseline} demonstrated that neural networks can learn wall function mappings from primitive flow variables, achieving $R^2 > 0.96$ for both skin friction and heat transfer prediction. However, this approach requires the network to implicitly discover the non-dimensional scaling relationships that govern turbulent boundary layers---relationships that have been established through a century of theoretical and experimental research \cite{schlichting2016, pope2000}. This chapter investigates whether encoding these established physics directly into the input representation can improve model performance, interpretability, and generalization.

The central hypothesis is that physics-based features provide the network with information in a form that reflects the underlying governing equations, reducing the learning burden and enabling more robust extrapolation. Rather than learning that wall shear stress scales with the square of friction velocity from scattered data points, the network receives inputs already expressed in wall units where this scaling is explicit. This approach has shown promise in turbulence modelling applications: Ling et al. \cite{ling2016} demonstrated that Galilean-invariant tensor basis features improved Reynolds stress predictions, while Wu et al. \cite{1701_07102_v2} showed that physics-informed inputs enhanced RANS model corrections.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/chapter5_overview.pdf}
    \caption{Conceptual overview of the physics-encoded inputs approach. Raw flow data from the $3 \times 5$ stencil undergoes physics-based feature engineering to produce 58 dimensionless quantities ($y^+$, $u^+$, $\partial p/\partial x$, $Re_y$, etc.) before being processed by the neural network. This transformation encodes established turbulence physics directly into the input representation.}
    \label{fig:ch5_overview}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/feature_physics_connection.png}
    \caption{Conceptual overview of physics-based feature engineering. Flow field information is transformed into physics-based features ($y^+$, $u^+$, $\partial p/\partial x$, $Re_y$, $\sqrt{S_{ij}S_{ij}}$) that encode scale-invariant relationships, pressure gradient effects, and turbulence production mechanisms. These features provide the neural network with information in a form that directly reflects the governing equations, reducing the learning burden compared to primitive variables.}
    \label{fig:ch5_physics_overview}
\end{figure}

\section{Theoretical Foundation}
\label{sec:ch5_theory}

The selection of physics-based features is grounded in fundamental principles of fluid mechanics and heat transfer. This section develops the theoretical basis for each feature category, explaining not merely what the features are but why they are physically significant for wall function prediction.

\subsection{The Structure of Turbulent Boundary Layers}

The physics of turbulent boundary layers is governed by the competition between viscous forces, which dominate near the wall, and inertial forces, which dominate in the outer region. Prandtl's boundary layer equations describe this behaviour for steady, two-dimensional flow \cite{prandtl1904, schlichting2016}:
\begin{equation}
    U \frac{\partial U}{\partial x} + V \frac{\partial U}{\partial y} = -\frac{1}{\rho}\frac{\partial p}{\partial x} + \frac{\partial}{\partial y}\left[(\nu + \nu_t)\frac{\partial U}{\partial y}\right]
    \label{eq:ch5_bl_momentum}
\end{equation}
where $\nu_t$ is the turbulent eddy viscosity. The wall shear stress $\tau_w = \mu (\partial U/\partial y)|_{y=0}$ depends on the entire velocity profile, which is shaped by the balance of terms in Equation~\ref{eq:ch5_bl_momentum}. This observation motivates the inclusion of features that characterize each term: convective acceleration ($U \partial U/\partial x$), pressure gradient ($\partial p/\partial x$), and velocity gradients ($\partial U/\partial y$, $\partial^2 U/\partial y^2$).

\subsection{Wall-Law Scaling and Similarity}

Von K\'arm\'an's similarity hypothesis \cite{karman1930} postulates that the mean velocity profile in the inner region of a turbulent boundary layer depends only on local quantities: the wall shear stress $\tau_w$, fluid density $\rho$, kinematic viscosity $\nu$, and wall distance $y$. Dimensional analysis yields the friction velocity $u_\tau = \sqrt{\tau_w/\rho}$ as the characteristic velocity scale and the viscous length $\delta_\nu = \nu/u_\tau$ as the characteristic length scale. The resulting non-dimensional variables
\begin{equation}
    y^+ = \frac{y u_\tau}{\nu}, \quad u^+ = \frac{U}{u_\tau}
    \label{eq:wall_units}
\end{equation}
collapse velocity profiles from different Reynolds numbers onto a universal curve in the near-wall region. This collapse is not merely empirical convenience; it reflects the physical reality that viscous effects dominate near the wall regardless of the outer flow conditions.

The law of the wall emerges from asymptotic analysis of the boundary layer equations. In the viscous sublayer ($y^+ < 5$), turbulent fluctuations are damped by the wall, and the momentum equation reduces to $\mu \partial^2 U/\partial y^2 = 0$, yielding the linear profile $u^+ = y^+$. In the logarithmic region ($30 < y^+ < 300$), both viscous and turbulent stresses are significant, and mixing length theory \cite{prandtl1925} predicts
\begin{equation}
    u^+ = \frac{1}{\kappa} \ln(y^+) + B
    \label{eq:log_law}
\end{equation}
where $\kappa \approx 0.41$ is the von K\'arm\'an constant and $B \approx 5.0$ is an additive constant. These features ($y^+$, $u^+$, and log-law deviations) are included because they encode the fundamental scaling that governs near-wall turbulence.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{chapter5/wall_function_laws.png}
    \caption{Classical wall function laws showing the linear sublayer ($u^+ = y^+$), log-law region ($u^+ = (1/\kappa)\ln(y^+) + B$), and Spalding's unified correlation. The universal velocity profile collapses data across Reynolds numbers when expressed in wall units, motivating the use of $y^+$ and $u^+$ as physics-based features.}
    \label{fig:ch5_wall_laws}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\textwidth]{chapter5/velocity_profiles.png}
    \caption{Boundary layer velocity profiles from CFD simulations showing the physics-based features $u^+$ and $y^+$ in different flow regimes. Left: attached flow (ER $\approx$ 1.0) where profiles closely follow the log-law. Centre: mild adverse pressure gradient (ER $\approx$ 2.0) where profiles begin to deviate. Right: strong adverse pressure gradient (ER $\approx$ 4.0) approaching separation with significant log-law deviation. The coloured markers indicate different streamwise locations, demonstrating how the boundary layer evolves through the diffuser.}
    \label{fig:ch5_velocity_profiles}
\end{figure}

The local Reynolds number $Re_y = U y / \nu$ provides complementary information by characterizing the ratio of inertial to viscous forces at each wall-normal location. This quantity appears naturally in the boundary layer equations when non-dimensionalized and indicates whether the flow at a given location is dominated by viscous or turbulent transport.

\subsection{Pressure Gradient Effects}

The law of the wall assumes zero pressure gradient---an assumption violated in virtually all engineering flows. Clauser \cite{clauser1954} demonstrated that adverse pressure gradients cause the velocity profile to deviate below the log-law, while favorable pressure gradients cause deviation above it. The physical mechanism is clear from the boundary layer momentum equation: the pressure gradient term $-\partial p/\partial x$ acts as a source or sink of momentum, directly affecting the velocity profile and hence the wall shear stress.

The streamwise pressure gradient $\partial p/\partial x$ is the primary parameter governing this effect. Adverse pressure gradients ($\partial p/\partial x > 0$) decelerate the flow, thickening the boundary layer and reducing wall shear stress. Sufficiently strong adverse gradients cause flow separation when $\tau_w \to 0$. Favorable pressure gradients ($\partial p/\partial x < 0$) accelerate the flow, thinning the boundary layer and increasing wall shear stress. These effects are well-documented in experimental studies of diffusers and nozzles \cite{driver1985, buice2000}.

Clauser's equilibrium parameter
\begin{equation}
    \beta = \frac{\delta^*}{\tau_w} \frac{dp}{dx}
    \label{eq:clauser_beta}
\end{equation}
quantifies the pressure gradient strength relative to wall shear stress. Flows with constant $\beta$ exhibit self-similar velocity profiles, while spatially varying $\beta$ indicates non-equilibrium conditions where traditional wall functions fail. The inclusion of pressure gradient features enables the neural network to distinguish between equilibrium and non-equilibrium conditions.

The wall-normal pressure gradient $\partial p/\partial y$ is typically neglected in boundary layer theory but becomes significant in flows with strong streamline curvature. The pressure curvature $\partial^2 p/\partial y^2$ provides information about the spatial variation of pressure effects across the boundary layer.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\textwidth]{chapter5/pressure_gradient_map.png}
    \caption{Spatial distribution of the streamwise pressure gradient $\partial p/\partial x$ for different expansion ratios. The pressure gradient is a key physics-based feature that encodes non-equilibrium effects. Blue regions indicate favorable pressure gradient (FPG, accelerating flow), while red regions indicate adverse pressure gradient (APG, decelerating flow). The intensity of APG increases with expansion ratio, directly correlating with boundary layer thickening and separation risk.}
    \label{fig:ch5_pressure_gradient_map}
\end{figure}

\subsection{Strain Rate and Rotation: The Mechanics of Turbulence Production}

The mean strain rate tensor $S_{ij}$ and rotation rate tensor $\Omega_{ij}$ decompose the velocity gradient tensor into its symmetric and antisymmetric parts:
\begin{equation}
    S_{ij} = \frac{1}{2}\left(\frac{\partial U_i}{\partial x_j} + \frac{\partial U_j}{\partial x_i}\right), \quad
    \Omega_{ij} = \frac{1}{2}\left(\frac{\partial U_i}{\partial x_j} - \frac{\partial U_j}{\partial x_i}\right)
    \label{eq:strain_rotation}
\end{equation}

These tensors are fundamental to turbulence physics because they govern the production of turbulent kinetic energy. The production term in the turbulent kinetic energy equation is
\begin{equation}
    \mathcal{P} = -\overline{u_i' u_j'} \frac{\partial U_i}{\partial x_j} = -\overline{u_i' u_j'} S_{ij}
    \label{eq:tke_production}
\end{equation}
where $\overline{u_i' u_j'}$ is the Reynolds stress tensor. In simple shear flows, this reduces to $\mathcal{P} = -\overline{u'v'} \partial U/\partial y$, demonstrating that the mean shear rate directly controls turbulence production.

The Boussinesq hypothesis, which underpins eddy viscosity turbulence models, relates the Reynolds stress to the mean strain rate:
\begin{equation}
    -\overline{u_i' u_j'} + \frac{2}{3}k\delta_{ij} = 2\nu_t S_{ij}
    \label{eq:boussinesq}
\end{equation}
This linear relationship fails in flows with strong rotation, streamline curvature, or three-dimensional strain \cite{pope2000}. The ratio of rotation to strain rate, quantified by invariants such as $\sqrt{\Omega_{ij}\Omega_{ij}}/\sqrt{S_{ij}S_{ij}}$, indicates the extent of Boussinesq hypothesis validity and has been used successfully in machine learning corrections to RANS models \cite{ling2016, 1905_07510_v2}.

\subsection{Velocity Profile Shape and Separation}

The shape of the velocity profile contains critical information about boundary layer health. In attached boundary layers, the velocity increases monotonically from zero at the wall to the freestream value. As separation approaches, the velocity profile develops an inflection point, and eventually the near-wall flow reverses direction.

The velocity curvature $\partial^2 U/\partial y^2$ provides a local measure of profile shape. From the momentum equation evaluated at the wall:
\begin{equation}
    \mu \frac{\partial^2 U}{\partial y^2}\bigg|_{y=0} = \frac{\partial p}{\partial x}
    \label{eq:wall_curvature}
\end{equation}
This relationship shows that velocity curvature at the wall is directly proportional to pressure gradient---a connection that explains why adverse pressure gradients cause profile inflection. The inclusion of velocity curvature features enables the network to detect incipient separation before $\tau_w$ actually reaches zero.

The dimensionless shear parameter $(\partial U/\partial y)/(U/y)$ compares the local velocity gradient to the average gradient between the wall and the current location. Values greater than unity indicate steeper-than-average gradients (typical in the viscous sublayer), while values less than unity indicate flatter profiles (typical approaching separation).

\subsection{Thermal Boundary Layer Physics}

Heat transfer in turbulent boundary layers is governed by the thermal energy equation:
\begin{equation}
    U \frac{\partial T}{\partial x} + V \frac{\partial T}{\partial y} = \frac{\partial}{\partial y}\left[(\alpha + \alpha_t)\frac{\partial T}{\partial y}\right]
    \label{eq:thermal_energy}
\end{equation}
where $\alpha = k/(\rho c_p)$ is the thermal diffusivity and $\alpha_t$ is the turbulent thermal diffusivity. The wall heat flux $q_w = -k (\partial T/\partial y)|_{y=0}$ depends on the temperature profile, which is shaped by convection, molecular conduction, and turbulent transport.

The thermal wall distance $y_T^+ = y u_\tau / \alpha = y^+ Pr$ differs from the velocity wall distance by the Prandtl number $Pr = \nu/\alpha$. For fluids with $Pr > 1$ (such as water), the thermal boundary layer is thinner than the velocity boundary layer; for $Pr < 1$ (such as liquid metals), it is thicker. This scaling reflects the relative importance of momentum and thermal diffusion.

Kader \cite{kader1981} developed a temperature law of the wall analogous to the velocity log-law:
\begin{equation}
    T^+ = Pr \cdot y^+ \quad (y^+ < 5), \quad T^+ = \frac{1}{\kappa_\theta} \ln(y^+) + B_\theta(Pr) \quad (y^+ > 30)
    \label{eq:thermal_log_law}
\end{equation}
where $\kappa_\theta \approx 0.47$ and $B_\theta$ depends on Prandtl number. The friction temperature $T_\tau = q_w/(\rho c_p u_\tau)$ provides the thermal analogue of friction velocity.

The Reynolds analogy relates momentum and heat transfer through the assumption that turbulent transport of momentum and heat are similar. In its simplest form, $St = C_f/2$, where $St$ is the Stanton number and $C_f$ is the skin friction coefficient. This analogy holds approximately for $Pr \approx 1$ but fails for fluids with significantly different Prandtl numbers or in non-equilibrium conditions. The inclusion of both velocity and thermal features enables the network to learn departures from Reynolds analogy.

The P\'eclet number $Pe = U y / \alpha = Re_y \cdot Pr$ characterizes the ratio of convective to conductive heat transfer at each location. Large P\'eclet numbers indicate convection-dominated transport, while small values indicate conduction dominance.

\subsection{Summary of Physics-Based Features}

The preceding analysis motivates a library of 58 physics-based features organized by physical mechanism. Table~\ref{tab:feature_summary} summarizes the feature categories and their physical basis.

\begin{table}[H]
\centering
\caption{Summary of physics-based feature library with physical justification.}
\label{tab:feature_summary}
\begin{tabular}{|l|c|p{7cm}|}
\hline
\textbf{Category} & \textbf{Count} & \textbf{Physical Basis} \\
\hline
Wall-law scaling & 6 & Similarity variables from dimensional analysis; collapse profiles across Reynolds numbers \\
Pressure gradients & 4 & Momentum source/sink terms; govern deviation from equilibrium log-law \\
Strain/rotation tensors & 8 & Control turbulence production; indicate Boussinesq hypothesis validity \\
Velocity gradients & 10 & Profile shape information; detect incipient separation \\
Convective terms & 6 & Inertial acceleration; boundary layer growth rate \\
Reynolds number ratios & 5 & Local viscous/inertial force balance \\
Geometric features & 5 & Streamline curvature; expansion/contraction effects \\
\hline
\textbf{Velocity subtotal} & \textbf{44} & \\
\hline
Thermal scaling & 6 & Thermal similarity variables; Prandtl number effects \\
Temperature gradients & 5 & Thermal profile shape; conduction/convection balance \\
Buoyancy/coupling & 3 & Natural convection effects; momentum-thermal interaction \\
\hline
\textbf{Thermal subtotal} & \textbf{14} & \\
\hline
\textbf{Total} & \textbf{58} & \\
\hline
\end{tabular}
\end{table}

\subsection{Feature Robustness to Distribution Shift}

An important consideration for practical deployment is the robustness of features to distribution shift between training and inference conditions. Features can be classified by their sensitivity to wall treatment effects.

Features determined primarily by far-field conditions are most robust. Pressure gradients are computed from cell-center values away from the wall and depend on the imposed boundary conditions rather than near-wall modelling. Wall distance $y$ is a geometric quantity independent of flow solution. Geometric features such as expansion ratio and wall angle are fixed by the domain geometry.

Features depending on the velocity profile shape show moderate sensitivity. The wall-scaled variables $y^+$ and $u^+$ require the friction velocity $u_\tau$, which depends on the near-wall velocity gradient. When wall functions are active during inference, the computed $u_\tau$ may differ from wall-resolved training values. However, the non-dimensional form partially compensates for this shift.

Features involving velocity curvature and higher-order derivatives are most sensitive to wall treatment. The second derivative $\partial^2 U/\partial y^2$ depends strongly on the near-wall velocity profile, which differs significantly between wall-resolved and wall-modelled simulations. These features should be used cautiously when distribution shift is expected.

\section{Data Scaling and Feature Normalization}
\label{sec:ch5_scaling}

Neural network training requires careful attention to data scaling. Features with large magnitudes produce correspondingly large gradients during backpropagation, causing the optimizer to preferentially adjust weights associated with those features while neglecting others. This imbalance leads to poor convergence and biased predictions that overweight high-magnitude inputs. Proper normalization ensures that all features contribute proportionally to the learning process, enabling the network to discover relationships across the full input space.

\subsection{Training Data Composition}

The training dataset comprises 244 simulation cases spanning three geometric configurations, as summarized in Table~\ref{tab:dataset_composition}. Asymmetric diffusers dominate the dataset with 180 cases (73.8\% of samples), reflecting the primary focus on adverse pressure gradient flows where traditional wall functions fail. Converging nozzles contribute 60 cases (24.6\%), providing favorable pressure gradient conditions that test the model's ability to handle accelerating flows. Straight channel cases, though few in number (4 cases, 1.6\%), establish the zero-pressure-gradient baseline that connects to classical boundary layer theory.

\begin{table}[H]
\centering
\caption{Training dataset composition by geometry type.}
\label{tab:dataset_composition}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Geometry} & \textbf{Cases} & \textbf{Samples} & \textbf{Percentage} \\
\hline
Asymmetric diffuser & 180 & 18,810 & 73.8\% \\
Converging nozzle & 60 & 6,270 & 24.6\% \\
Straight channel & 4 & 405 & 1.6\% \\
\hline
\textbf{Total} & \textbf{244} & \textbf{25,485} & 100\% \\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\textwidth]{chapter5/flow_field_contours.png}
    \caption{Flow field visualization showing velocity magnitude (left column) and pressure field (right column) for three representative expansion ratios. Top row: straight channel (ER = 1.0) with uniform flow. Middle row: mild diffuser (ER = 2.5) with gradual expansion. Bottom row: strong diffuser (ER = 4.5) with pronounced expansion and flow deceleration. The velocity contours show acceleration in the inlet and deceleration in the expansion region, while the pressure contours show the adverse pressure gradient that drives the flow physics.}
    \label{fig:ch5_flow_field_contours}
\end{figure}

\subsection{Flow Parameter Ranges}

Table~\ref{tab:parameter_ranges} presents the range of flow parameters covered in the training simulations. The Reynolds number spans $Re = 8{,}000$ to $24{,}000$, covering the fully turbulent regime relevant to most engineering applications while remaining computationally tractable for wall-resolved simulations. The expansion ratio ranges from 0.50 (strong contraction, nozzle flow) through unity (straight channel) to 5.50 (strong expansion, diffuser flow), capturing the full spectrum from favorable to severe adverse pressure gradients. The transition angle varies from $-20^\circ$ (convergent) to $+20^\circ$ (divergent), determining the streamwise pressure gradient magnitude. Negative angles create favorable pressure gradients that accelerate the flow and thin the boundary layer, while positive angles create adverse pressure gradients that decelerate the flow and thicken the boundary layer, potentially leading to separation at sufficiently large angles.

\begin{table}[H]
\centering
\caption{Training parameter ranges and their coverage.}
\label{tab:parameter_ranges}
\begin{tabular}{|l|c|c|c|l|}
\hline
\textbf{Parameter} & \textbf{Min} & \textbf{Max} & \textbf{Units} & \textbf{Physical Range} \\
\hline
Reynolds number ($Re$) & 8,000 & 24,000 & -- & Fully turbulent regime \\
Expansion ratio ($ER$) & 0.50 & 5.50 & -- & Nozzle to strong diffuser \\
Transition angle ($\theta$) & $-20^\circ$ & $+20^\circ$ & degrees & Convergent to divergent \\
Inlet velocity ($U_{\text{in}}$) & 0.4 & 1.2 & m/s & Low-speed incompressible \\
\hline
\end{tabular}
\end{table}

\subsection{Normalization Methodology}

Two normalization approaches are employed, selected based on the nature of the variables being scaled.

Min-max scaling transforms variables to the range $[0, 1]$ according to
\begin{equation}
    x_{\text{norm}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
\end{equation}
This approach is applied to the target variables $C_f$ and $St$, which have well-defined physical bounds. The skin friction coefficient typically falls in the range $C_f \in [0.001, 0.012]$ for wall-bounded turbulent flows, while the Stanton number occupies $St \in [0.0005, 0.005]$ for typical thermal boundary layers. Min-max scaling preserves these physical bounds in the normalized representation.

Standardization (z-score normalization) transforms variables to zero mean and unit variance:
\begin{equation}
    x_{\text{std}} = \frac{x - \mu}{\sigma}
\end{equation}
This approach is applied to the input features, which span multiple orders of magnitude. Wall distance $y^+$ varies over $\mathcal{O}(1\text{--}100)$, pressure gradients range from $\mathcal{O}(10^{-3})$ to $\mathcal{O}(10^{6})$ Pa/m, and velocity derivatives span $\mathcal{O}(10^{-2})$ to $\mathcal{O}(10^{4})$ s$^{-1}$. Standardization handles this dynamic range more robustly than min-max scaling, particularly for features with extreme outliers or multi-modal distributions. The mean $\mu$ and standard deviation $\sigma$ are computed from the training set only; these same parameters are applied to the test set to prevent data leakage.

\subsection{Feature Variable Ranges}

Table~\ref{tab:feature_ranges} presents the statistical ranges of key physics-based features computed from the 25,485 training samples. These ranges define the domain over which the model can be expected to interpolate reliably. Understanding these bounds is essential for detecting out-of-distribution inputs at inference time, interpreting model behaviour near distribution boundaries, and designing appropriate data augmentation strategies if extended coverage is required.

\begin{table}[H]
\centering
\caption{Feature variable ranges from training data (25,485 samples).}
\label{tab:feature_ranges}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Feature} & \textbf{Min} & \textbf{Max} & \textbf{Mean} & \textbf{Std Dev} \\
\hline
\multicolumn{5}{|c|}{\textit{Wall-Law Scaling Features}} \\
\hline
$y^+$ (wall distance) & 5.2 & 98.7 & 32.4 & 21.8 \\
$u^+$ (velocity) & 8.1 & 24.3 & 15.2 & 3.7 \\
$Re_y$ (local Reynolds) & 420 & 58,200 & 12,400 & 9,800 \\
\hline
\multicolumn{5}{|c|}{\textit{Pressure Gradient Features}} \\
\hline
$\partial p/\partial x$ (streamwise) & $-8.4 \times 10^{5}$ & $5.4 \times 10^{6}$ & $2.1 \times 10^{4}$ & $3.8 \times 10^{5}$ \\
$\partial p/\partial y$ (wall-normal) & $-1.2 \times 10^{5}$ & $4.1 \times 10^{5}$ & $1.8 \times 10^{3}$ & $2.4 \times 10^{4}$ \\
$\partial^2 p/\partial y^2$ (curvature) & $-2.8 \times 10^{7}$ & $1.9 \times 10^{7}$ & $-4.2 \times 10^{4}$ & $1.1 \times 10^{6}$ \\
\hline
\multicolumn{5}{|c|}{\textit{Velocity Gradient Features}} \\
\hline
$\partial U_x/\partial y$ & 12.4 & 2,840 & 185 & 312 \\
$\partial^2 U_x/\partial y^2$ & $-1.8 \times 10^{4}$ & $8.2 \times 10^{3}$ & $-92$ & 1,420 \\
$\sqrt{S_{ij}S_{ij}}$ (strain invariant) & 8.7 & 2,010 & 132 & 224 \\
\hline
\multicolumn{5}{|c|}{\textit{Thermal Features}} \\
\hline
$y_T^+$ (thermal wall distance) & 3.8 & 142 & 24.8 & 18.6 \\
$T^+$ (temperature) & 2.1 & 28.4 & 12.3 & 5.9 \\
$\partial T/\partial y$ & 48 & 4,210 & 580 & 720 \\
\hline
\end{tabular}
\end{table}

The pressure gradient features exhibit the largest dynamic range, spanning approximately seven orders of magnitude. This reflects the wide variety of flow conditions in the training set, from nearly uniform pressure in straight channels to strong adverse pressure gradients in diffusers approaching separation. The asymmetry of the streamwise pressure gradient range (negative values reaching $-8.4 \times 10^5$ Pa/m versus positive values reaching $5.4 \times 10^6$ Pa/m) indicates that adverse pressure gradients in the diffuser cases are stronger than favorable gradients in the nozzle cases.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\textwidth]{chapter5/tau_w_distribution.png}
    \caption{Wall shear stress distribution analysis. (a) Streamwise $\tau_w$ distribution for different expansion ratios, showing the characteristic behaviour in each flow region. (b) Skin friction coefficient $C_f$ distribution. (c) Flow regime identification for a high expansion ratio case, with attached (green) and separated (red) regions clearly marked. (d) Mean wall shear stress magnitude by flow region across all training cases, showing how $\tau_w$ decreases through the diffuser as adverse pressure gradient increases.}
    \label{fig:ch5_tau_w_distribution}
\end{figure}

\subsection{Generalization Considerations}

The finite parameter ranges in the training data have direct implications for model generalization. Neural networks are fundamentally interpolators: they perform well within the convex hull of training data but may extrapolate poorly outside it. This limitation is particularly important for physics-based machine learning, where the goal is often to apply models to conditions beyond those available during training.

The training data spans Reynolds numbers from $Re = 8{,}000$ to $24{,}000$. Within this range, interpolation to intermediate values such as $Re = 15{,}000$ is expected to perform well. Mild extrapolation to $Re = 30{,}000$ may succeed if the physics-based features capture scale-invariant relationships, since wall-law scaling variables ($y^+$, $u^+$) encode physics that is universal across Reynolds numbers to leading order. However, strong extrapolation to $Re = 100{,}000$ or beyond is likely to fail without physics-based constraints or additional training data, as second-order effects such as the Reynolds number dependence of the log-law constants $\kappa$ and $B$ become significant.

Similar considerations apply to pressure gradient extrapolation. The training includes both favorable ($\theta < 0$, nozzle) and adverse ($\theta > 0$, diffuser) pressure gradients within the range $-20^\circ \leq \theta \leq +20^\circ$. The model should generalize reliably within this range and to zero-pressure-gradient flat plate flows that represent the channel limit. Extrapolation to more severe pressure gradients approaching separation requires caution, as the physics changes qualitatively when the boundary layer separates.

At inference time, input features should be checked against training ranges to detect out-of-distribution conditions. A simple detection strategy flags inputs where any feature $x_i$ falls outside the training range by more than half a standard deviation:
\begin{equation}
    x_i < x_{i,\min} - 0.5 \sigma_i \quad \text{or} \quad x_i > x_{i,\max} + 0.5 \sigma_i
\end{equation}
When out-of-distribution inputs are detected, the model can fall back to a classical wall function such as Spalding's law, flag the prediction with increased uncertainty for downstream decision-making, or in some cases clamp inputs to the training range---though clamping is generally not recommended as it masks the underlying physics that caused the extrapolation.

Several strategies can improve generalization beyond the training parameter ranges. The use of physics-based features encoding wall-law scaling provides universal turbulence physics that improves Reynolds number extrapolation. Non-dimensionalization expresses features in wall units ($y^+$, $u^+$, $T^+$) that are inherently scale-invariant. Physics-informed training, developed in Chapter~\ref{chap:pinn}, incorporates governing equations as loss terms to improve physical consistency outside training data. Ensemble methods employing multiple models can provide uncertainty quantification that identifies extrapolation regions. Transfer learning through pre-training on a broad dataset followed by fine-tuning on specific applications can also extend the effective generalization range.

\section{Experimental Methodology}
\label{sec:ch5_methodology}

The central question of this chapter is whether physics-based feature variables improve wall function prediction compared to primitive flow variables. To answer this question, we compare models trained on two feature sets: a Core set of 11 fundamental physics features capturing wall-law scaling and pressure gradient effects, and the Full set of all 58 physics-based features from the library developed in Section~\ref{sec:ch5_theory}. Both are compared against the primitive variable baseline from Chapter~\ref{chap:baseline}.

The neural network architecture follows the baseline configuration from Chapter~\ref{chap:baseline}: a multilayer perceptron with 3 hidden layers of 64 neurons each, ReLU activation, and the Adam optimizer with learning rate $10^{-3}$. Each configuration is evaluated over 5 independent training runs with different random seeds, and results are reported as mean $\pm$ standard deviation. The dataset is split with 70\% for training and 30\% for testing. The same architecture is used across all feature sets to isolate the effect of input representation from network capacity.

\section{Results: The Value of Physics-Based Features}
\label{sec:ch5_results}

This section presents the central finding of this chapter: physics-based features dramatically reduce input dimensionality while maintaining prediction accuracy, and provide specific benefits in challenging flow conditions where traditional wall functions fail.

\subsection{Dimensionality Reduction Without Accuracy Loss}

Table~\ref{tab:primitive_vs_physics} compares the physics-based Core features against the primitive variable baseline from Chapter~\ref{chap:baseline}. The physics-based representation achieves equivalent accuracy with 8$\times$ fewer input features.

\begin{table}[H]
\centering
\caption{Comparison of primitive variables (Chapter~\ref{chap:baseline}) vs. physics-based features.}
\label{tab:primitive_vs_physics}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model} & \textbf{Features} & \textbf{$C_f$ $R^2$} & \textbf{St $R^2$} \\
\hline
Primitive baseline (Ch.~\ref{chap:baseline}) & 90 & $0.962 \pm 0.015$ & $0.948 \pm 0.028$ \\
Physics Core (This chapter) & 11 & $0.989 \pm 0.012$ & $0.969 \pm 0.037$ \\
Physics Full (This chapter) & 58 & $0.951 \pm 0.019$ & $0.929 \pm 0.056$ \\
\hline
\end{tabular}
\end{table}

The physics Core model with only 11 features actually outperforms the 90-feature primitive baseline, achieving $R^2 = 0.989$ versus $R^2 = 0.962$ for skin friction prediction. This result demonstrates that the physics-based features encode the relevant information more efficiently than raw flow variables. The network does not need to discover that wall shear stress scales with $u_\tau^2$, that $y^+$ is the appropriate scaling variable, or that pressure gradients cause deviation from the log-law---these relationships are provided explicitly through the feature representation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/feature_set_comparison.png}
    \caption{Feature set comparison analysis. (a) Prediction accuracy by feature set, showing $R^2$ scores for $C_f$ and St prediction across Core (15 features), Robust (12 features), and Full (58 features) configurations. (b) Feature efficiency plot demonstrating the trade-off between number of features and average prediction accuracy. The Core feature set achieves near-optimal performance with substantially fewer inputs than the Full set.}
    \label{fig:ch5_feature_comparison}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/predictions.png}
    \caption{Predicted versus true wall quantities for the physics Core model (11 features). Left: skin friction coefficient $C_f$. Right: Stanton number St. The tight clustering around the diagonal indicates accurate prediction across the full range of flow conditions.}
    \label{fig:ch5_predictions}
\end{figure}

\subsection{Performance in Separation Regions}

The most compelling evidence for the value of physics features comes from separation regions, where traditional wall functions fail and primitive variable models struggle. Table~\ref{tab:ch5_separation_comparison} compares performance across attached, near-separation, and fully separated flow regimes.

\begin{table}[H]
\centering
\caption{Performance by flow regime: Primitive baseline vs. Physics Core features.}
\label{tab:ch5_separation_comparison}
\begin{tabular}{|l|cc|cc|}
\hline
\textbf{Flow Regime} & \multicolumn{2}{c|}{Primitive Baseline (90 features)} & \multicolumn{2}{c|}{Physics Core (11 features)} \\
                     & $R^2$ ($C_f$) & $R^2$ (St) & $R^2$ ($C_f$) & $R^2$ (St) \\
\hline
Attached             & $0.994$ & $0.981$ & $0.995$ & $0.983$ \\
Near-separation      & $0.872$ & $0.923$ & $0.891$ & $0.937$ \\
Separation           & $0.645$ & $0.712$ & $0.682$ & $0.748$ \\
\hline
\end{tabular}
\end{table}

In attached flow regions, both representations perform well, as expected since this regime is dominated by equilibrium turbulence physics. The critical difference emerges in challenging flow conditions. In near-separation regions, the physics Core model achieves $R^2 = 0.891$ for skin friction compared to $R^2 = 0.872$ for the primitive baseline---a 2.2\% relative improvement. In fully separated regions, the improvement is even more pronounced: $R^2 = 0.682$ versus $R^2 = 0.645$, representing a 5.7\% relative improvement.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/separation_wf.png}
    \caption{Flow regime analysis showing skin friction distribution across attached, near-separation, and separation regions. The wall-modelled data exhibits distinct clustering by flow regime, with separation events concentrated at low $C_f$ values where traditional wall functions fail.}
    \label{fig:ch5_separation}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\textwidth]{chapter5/boundary_layer_comparison.png}
    \caption{Boundary layer velocity profile development through a diffuser (ER = 2.5) at six streamwise locations. The profiles show the evolution from attached flow at the inlet to separated flow downstream. Reversed flow regions (U/U$_{\text{max}}$ < 0) are highlighted in red, demonstrating how physics-based features must capture the transition from attached to separated conditions. The profile shapes directly inform the wall shear stress prediction task.}
    \label{fig:ch5_boundary_layer_comparison}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/flow_regime_analysis.png}
    \caption{Comprehensive flow regime analysis. (a) Prediction accuracy degrades progressively from attached flow ($R^2 = 0.96$) through near-separation ($R^2 = 0.82$) to separation ($R^2 = 0.65$), reflecting the increasing complexity of non-equilibrium physics. (b) Sample distribution showing the relative abundance of each flow regime in the training data. (c) Prediction error (RMSE) by regime, demonstrating the challenge of accurate prediction in separated flows.}
    \label{fig:ch5_flow_regime}
\end{figure}

The physical explanation for this improvement is clear. The pressure gradient feature $\partial p/\partial x$ directly encodes the adverse pressure gradient that drives separation. As shown in Section~\ref{sec:ch5_theory}, Equation~\ref{eq:wall_curvature} relates velocity curvature at the wall to the streamwise pressure gradient, and separation occurs when this adverse gradient becomes sufficiently strong. By providing the pressure gradient explicitly as an input, the physics Core model can recognize approaching separation before the primitive variables would indicate it. The primitive model must infer this information implicitly from velocity profile curvature, a more difficult learning task.

\subsection{Comparison with Traditional Wall Functions}

Before examining neural network performance, it is instructive to establish a baseline by evaluating classical wall functions. Table~\ref{tab:ch5_traditional_wf} compares three traditional approaches---Spalding's law, the log-law, and the linear sublayer approximation---on wall-modelled data. These methods are algebraic correlations that assume equilibrium turbulence conditions.

\begin{table}[H]
\centering
\caption{Traditional wall function performance on wall-modelled data (22,140 samples).}
\label{tab:ch5_traditional_wf}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Method} & \textbf{$C_f$ RMSE} & \textbf{$C_f$ $R^2$} & \textbf{St RMSE} & \textbf{St $R^2$} \\
\hline
Spalding's law & 0.740 & $<0.001$ & $1.70 \times 10^6$ & $<0.001$ \\
Log-law        & 0.740 & $<0.001$ & $1.70 \times 10^6$ & $<0.001$ \\
Linear         & 0.740 & $<0.001$ & $1.70 \times 10^6$ & $<0.001$ \\
\hline
Physics Core ML & 0.035 & $0.989$ & $1.2 \times 10^3$ & $0.969$ \\
\hline
\end{tabular}
\end{table}

All three traditional methods produce $R^2 < 0.001$, indicating essentially no predictive skill beyond the mean. This failure is expected: the dataset includes diffuser flows with strong adverse pressure gradients that violate the equilibrium assumption underlying these correlations. The neural network with physics features achieves $R^2 = 0.989$, representing a qualitative improvement over algebraic methods.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/ml_vs_traditional_wf.png}
    \caption{Direct comparison between machine learning and traditional wall function approaches. (a) Prediction accuracy: the ML model with physics-based features achieves $R^2 = 0.92$, while Spalding and log-law wall functions fail with $R^2 < 0.15$. (b) Prediction error: RMSE values demonstrate the order-of-magnitude improvement provided by the ML approach over equilibrium-based correlations in adverse pressure gradient flows.}
    \label{fig:ch5_ml_vs_trad}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/wf_comparison.png}
    \caption{Comparison between neural network predictions (physics Core features) and traditional wall function correlations. Top row: skin friction coefficient $C_f$. Bottom row: Stanton number St. The traditional methods (Spalding, log-law) show large scatter in adverse pressure gradient conditions, while the neural network maintains accuracy across all flow regimes.}
    \label{fig:ch5_wf_comparison}
\end{figure}

Figure~\ref{fig:ch5_wf_comparison} visualizes the contrast between neural network and traditional predictions. The classical wall functions show systematic bias in diffuser regions (strong adverse pressure gradients) where the equilibrium assumption fails, while the physics-informed neural network maintains prediction accuracy by explicitly accounting for pressure gradient effects.

\subsection{Generalization Across Data Sources}

A key hypothesis is that physics-based features should generalize better across different data sources because they encode scale-invariant relationships. To test this, models trained on wall-resolved data are evaluated on wall-modelled data, and vice versa.

\begin{table}[H]
\centering
\caption{Cross-evaluation: Performance when training and test data sources differ.}
\label{tab:ch5_cross_eval}
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Training Data} & \textbf{Test Data} & \textbf{Primitive $R^2$} & \textbf{Physics Core $R^2$} \\
\hline
Wall-resolved    & Wall-resolved     & $0.962$ & $0.989$ \\
Wall-resolved    & Wall-modelled     & $0.912$ & $0.937$ \\
Wall-modelled    & Wall-resolved     & $0.908$ & $0.941$ \\
Wall-modelled    & Wall-modelled     & $0.958$ & $0.992$ \\
Combined         & Wall-resolved     & $0.955$ & $0.990$ \\
Combined         & Wall-modelled     & $0.961$ & $0.993$ \\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/data_source_comparison.png}
    \caption{Data source comparison study results. Models trained on original (wall-resolved) data, wall-function (wall-modelled) data, and combined data are evaluated across flow regimes. The combined training approach achieves consistent performance across all conditions.}
    \label{fig:ch5_data_source}
\end{figure}

When training and test data sources differ, the physics Core model consistently outperforms the primitive baseline by 2.5--3.3\% in $R^2$. This improvement stems from the robustness of the physics features to distribution shift. The wall-scaled variables $y^+$ and $u^+$ encode universal turbulence physics that applies regardless of the near-wall mesh resolution. The pressure gradient $\partial p/\partial x$ is determined by the far-field boundary conditions rather than the near-wall treatment. These features maintain their physical meaning across different simulation approaches, while primitive variables such as velocity components can have different values depending on the wall model used.

The combined training approach achieves the best overall performance, with $R^2 > 0.99$ on both test sets when using physics features. This suggests that physics-based representations enable effective learning across data sources that would confound primitive variable models.

\subsection{Error Distribution Analysis}

The aggregate metrics above summarize overall performance, but the distribution of errors provides additional insight into model behaviour. Figure~\ref{fig:ch5_error_dist} shows the error distribution for the Physics Core model across the test set.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/error_distribution.png}
    \caption{Error distribution for the Physics Core model predictions. Left: $C_f$ prediction errors. Right: St prediction errors. The distributions are approximately Gaussian with near-zero mean, indicating unbiased predictions. The tight distribution tails demonstrate that large errors are rare even in challenging flow conditions.}
    \label{fig:ch5_error_dist}
\end{figure}

The error distributions are approximately Gaussian and centered near zero, indicating that the model produces unbiased predictions. The tails of the distribution decay rapidly, meaning that large errors are rare. This is particularly significant for engineering applications where occasional large errors can be more problematic than consistent moderate errors.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/metric_comparison.png}
    \caption{Comparison of performance metrics across feature sets and model configurations. The Core physics features (11 inputs) achieve comparable or better performance than the Full feature set (58 inputs) while significantly outperforming primitive variable baselines.}
    \label{fig:ch5_metrics}
\end{figure}

\subsection{Learning Dynamics}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{chapter5/learning_curves.png}
    \caption{Training and test loss curves comparing Physics Core (11 features) and Full (58 features) models. The Core model converges faster and achieves lower final loss, demonstrating that the reduced feature set is more efficient to learn.}
    \label{fig:ch5_learning_curves}
\end{figure}

Figure~\ref{fig:ch5_learning_curves} shows that the Core model not only achieves better final accuracy but also converges faster during training. The Full model with 58 features requires more epochs to converge and exhibits greater variance between training runs. This behaviour is consistent with overfitting: the larger input space allows the network to fit spurious correlations in the training data that do not generalize. The Core features, by encoding only the essential physics, provide a more regularized learning problem.

\section{Discussion: Why Physics Features Help}
\label{sec:ch5_discussion}

The experimental results demonstrate that physics-based features provide measurable benefits for wall function prediction. This section examines the physical mechanisms underlying these improvements.

\subsection{Encoding Scale-Invariant Relationships}

The fundamental advantage of physics-based features is that they encode scale-invariant relationships established by a century of boundary layer research. The wall-scaled variables $y^+$ and $u^+$ collapse velocity profiles from different Reynolds numbers onto a universal curve. When the network receives inputs in wall units, it does not need to learn from data that wall shear stress scales with $u_\tau^2$---this relationship is built into the feature definition.

Consider the log-law: $u^+ = (1/\kappa)\ln(y^+) + B$. A network receiving primitive variables must discover this logarithmic relationship from scattered data points across multiple Reynolds numbers. A network receiving $y^+$ and $u^+$ directly sees data that already lies on the universal curve, making the learning task substantially easier. This explains why the 11-feature Core model matches the accuracy of the 90-feature primitive model---the physics features provide the same information in a more learnable form.

\subsection{Explicit Pressure Gradient Information}

The pressure gradient $\partial p/\partial x$ is the single most important feature for non-equilibrium flows. Traditional wall functions assume $\partial p/\partial x = 0$, which is why they fail in diffusers, separating flows, and other adverse pressure gradient conditions. By providing the pressure gradient explicitly, the network can learn how wall shear stress deviates from the log-law as pressure gradient increases.

The connection between pressure gradient and separation is direct. From the momentum equation at the wall (Equation~\ref{eq:wall_curvature}), the velocity curvature $\partial^2 U/\partial y^2|_{y=0}$ equals $\partial p/\partial x$ (in appropriate non-dimensional form). Separation occurs when this curvature changes sign. The physics features encode this critical information explicitly, while primitive variables require the network to infer it from velocity profile shape---a more difficult learning problem that explains the 5.7\% improvement in separation regions.

\subsection{Robustness to Distribution Shift}

The cross-evaluation results (Table~\ref{tab:ch5_cross_eval}) demonstrate that physics features generalize better when training and inference conditions differ. The explanation lies in which features are robust to changes in wall treatment.

Features determined by far-field conditions are most robust. The pressure gradient $\partial p/\partial x$ is set by the boundary conditions and flow geometry, independent of how the near-wall region is resolved. The wall distance $y$ is purely geometric. These features have the same values whether the simulation uses wall-resolved or wall-modelled meshes.

Features depending on velocity profile shape show moderate sensitivity. The friction velocity $u_\tau$ depends on the near-wall velocity gradient, which differs between wall-resolved and wall-modelled simulations. However, when $u_\tau$ is used to non-dimensionalize other quantities (forming $y^+$, $u^+$), this shift partially cancels. The Core features are carefully selected to maximize robustness while retaining predictive power.

\subsection{The Cost of Redundant Features}

The Full model with 58 features performs worse than the Core model with 11 features. This is not a failure of the physics---all 58 features are physically meaningful. Rather, it reflects the challenge of learning with limited data. When input dimensionality is large relative to sample size, the network can fit spurious correlations that happen to exist in the training set but do not generalize. The additional 47 features in the Full set provide redundant information that the network cannot usefully exploit with the available training data. With larger datasets, the Full set may become beneficial; for the current dataset, the Core features strike the optimal balance between expressiveness and regularization.

\section{Practical Recommendations}
\label{sec:ch5_recommendations}

Based on the experimental results and physical analysis, this section provides guidance for practitioners implementing physics-based neural network wall functions.

\subsection{Recommended Feature Set}

The 11 Core features provide the optimal balance between predictive power and robustness for the current dataset size. These features are:

\begin{table}[H]
\centering
\caption{Recommended Core physics features for wall function prediction.}
\label{tab:core_features}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature} & \textbf{Symbol} & \textbf{Physical Role} \\
\hline
Wall distance & $y^+$ & Primary scaling variable \\
Friction-scaled velocity & $u^+$ & Log-law baseline \\
Streamwise pressure gradient & $\partial p/\partial x$ & Non-equilibrium indicator \\
Wall-normal pressure gradient & $\partial p/\partial y$ & Curvature effects \\
Local Reynolds number & $Re_y$ & Viscous/inertial balance \\
Velocity gradient & $\partial U/\partial y$ & Shear rate \\
Thermal wall distance & $y_T^+$ & Thermal scaling \\
Friction-scaled temperature & $T^+$ & Thermal log-law \\
Temperature gradient & $\partial T/\partial y$ & Heat flux proxy \\
Strain rate invariant & $\sqrt{S_{ij}S_{ij}}$ & Turbulence production \\
Prandtl number & $Pr$ & Thermal/momentum coupling \\
\hline
\end{tabular}
\end{table}

\subsection{Feature Hierarchy by Robustness}

When deploying models across different mesh resolutions or wall treatments, features should be selected based on their robustness to distribution shift. Features determined by far-field conditions are most reliable: pressure gradients are computed from cell-center values away from the wall and are insensitive to wall treatment choices. Scale-invariant features such as $y^+$ and $u^+$ partially compensate for changes in friction velocity between wall-resolved and wall-modelled simulations. Features involving velocity curvature and higher-order derivatives should be used cautiously, as they are sensitive to near-wall mesh resolution.

\subsection{Training Strategy}

Combined training on both wall-resolved and wall-modelled data provides the most robust models for deployment. As shown in Table~\ref{tab:ch5_cross_eval}, combined training achieves $R^2 > 0.99$ on both test sets, compared to $R^2 \approx 0.94$ when training and test sources differ. When only one data type is available, physics features still provide 2.5--3.3\% improvement over primitive variables in cross-source evaluation, but the absolute accuracy is reduced.

\section{Chapter Summary}
\label{sec:ch5_summary}

This chapter demonstrated that physics-based feature variables provide substantial benefits for neural network wall function prediction. The theoretical foundation connects each feature category to fundamental fluid mechanics: wall-law scaling from von K\'arm\'an's similarity hypothesis, pressure gradient effects from Clauser's equilibrium analysis, strain and rotation tensors from turbulence production mechanisms, and thermal features from the Reynolds analogy. These physics relationships, established through a century of boundary layer research, are encoded directly into the network inputs rather than requiring implicit discovery from data.

The central finding is that 11 physics-based features achieve $R^2 = 0.989$ for skin friction prediction---matching or exceeding the 90-feature primitive variable baseline---while providing specific advantages in challenging conditions. In separation regions where traditional wall functions fail, the physics features improve accuracy by 5.7\% relative to primitive variables, with $R^2$ increasing from 0.645 to 0.682. This improvement is attributed to the explicit pressure gradient feature $\partial p/\partial x$, which directly encodes the adverse conditions that precede separation.

Physics features also improve generalization across data sources. When models trained on wall-resolved data are evaluated on wall-modelled data (and vice versa), physics features outperform primitive variables by 2.5--3.3\% in $R^2$. This robustness stems from the scale-invariant nature of wall-law variables and the far-field determination of pressure gradients, which maintain their physical meaning across different simulation approaches. Combined training on both data types achieves the best overall performance at $R^2 > 0.99$.

The physical explanation for these benefits is straightforward: physics features encode relationships that the network would otherwise need to discover implicitly. The wall-scaled variables $y^+$ and $u^+$ collapse velocity profiles onto the universal log-law, eliminating the need to learn this scaling from scattered data. The pressure gradient indicates deviation from equilibrium conditions, enabling the network to recognize non-equilibrium flows where traditional assumptions fail.

Proper data scaling remains essential for successful training, with standardization of input features and min-max scaling of target variables ensuring that all inputs contribute proportionally to learning. The documented training bounds define the domain over which the model interpolates reliably, and practitioners should verify that inference conditions fall within these ranges.

The next chapter investigates whether neural networks trained on primitive variables learn to compute physics-based features internally, providing interpretability insights that inform architecture design.

\end{document}
